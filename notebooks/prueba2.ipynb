{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   uid  d   t  x   y\n",
      "0    0  0   0  0  40\n",
      "1    0  0   1  1  41\n",
      "2    0  1  12  2  42\n",
      "3    0  1  13  3  43\n",
      "4    0  2  24  4  44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Datos de prueba\n",
    "data = {\n",
    "    \"uid\": [0] * 20 + [1] * 20,  # Usuarios: 0, 1\n",
    "    \"d\": [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9] * 2,\n",
    "    \"t\": [0, 1, 12, 13, 24, 25, 36, 37, 41, 47, 0, 1, 12, 13, 24, 25, 36, 37, 41, 47]\n",
    "    * 2,\n",
    "    \"x\": range(40),\n",
    "    \"y\": range(40, 80),\n",
    "}\n",
    "\n",
    "\n",
    "# Configuración inicial\n",
    "# num_users = 3\n",
    "# num_pings = 40\n",
    "\n",
    "# data = {\n",
    "#     \"uid\": np.repeat(np.arange(num_users), num_pings),  # Usuarios: 0, 1, 2\n",
    "#     \"d\": np.repeat(np.arange(num_pings) // 5, num_users),  # Días crecientes por cada 5 pings\n",
    "#     \"t\": np.tile(np.arange(num_pings) % 48, num_users),  # Timeslots crecientes en ciclos de 48\n",
    "#     \"x\": np.random.randint(0, 200, num_users * num_pings),  # Coordenadas X aleatorias\n",
    "#     \"y\": np.random.randint(0, 200, num_users * num_pings),  # Coordenadas Y aleatorias\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# Crear DataFrame\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>d</th>\n",
       "      <th>t</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>t_unificado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>54</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>57</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>58</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>19</td>\n",
       "      <td>59</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>64</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>65</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>66</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>67</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>68</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>29</td>\n",
       "      <td>69</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>71</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>72</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>73</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>74</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>75</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>76</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>77</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>38</td>\n",
       "      <td>78</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>39</td>\n",
       "      <td>79</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid  d   t   x   y  t_unificado\n",
       "0     0  0   0   0  40            0\n",
       "1     0  0   1   1  41            1\n",
       "2     0  1  12   2  42           60\n",
       "3     0  1  13   3  43           61\n",
       "4     0  2  24   4  44          120\n",
       "5     0  2  25   5  45          121\n",
       "6     0  3  36   6  46          180\n",
       "7     0  3  37   7  47          181\n",
       "8     0  4  41   8  48          233\n",
       "9     0  4  47   9  49          239\n",
       "10    0  5   0  10  50          240\n",
       "11    0  5   1  11  51          241\n",
       "12    0  6  12  12  52          300\n",
       "13    0  6  13  13  53          301\n",
       "14    0  7  24  14  54          360\n",
       "15    0  7  25  15  55          361\n",
       "16    0  8  36  16  56          420\n",
       "17    0  8  37  17  57          421\n",
       "18    0  9  41  18  58          473\n",
       "19    0  9  47  19  59          479\n",
       "20    1  0   0  20  60            0\n",
       "21    1  0   1  21  61            1\n",
       "22    1  1  12  22  62           60\n",
       "23    1  1  13  23  63           61\n",
       "24    1  2  24  24  64          120\n",
       "25    1  2  25  25  65          121\n",
       "26    1  3  36  26  66          180\n",
       "27    1  3  37  27  67          181\n",
       "28    1  4  41  28  68          233\n",
       "29    1  4  47  29  69          239\n",
       "30    1  5   0  30  70          240\n",
       "31    1  5   1  31  71          241\n",
       "32    1  6  12  32  72          300\n",
       "33    1  6  13  33  73          301\n",
       "34    1  7  24  34  74          360\n",
       "35    1  7  25  35  75          361\n",
       "36    1  8  36  36  76          420\n",
       "37    1  8  37  37  77          421\n",
       "38    1  9  41  38  78          473\n",
       "39    1  9  47  39  79          479"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"t_unificado\"] = df[\"d\"] * 48 + df[\"t\"]\n",
    "df = df.sort_values(by=[\"uid\", \"t_unificado\"]).reset_index(drop=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.059248608)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_tensor_from_data(data, grid_size=200):\n",
    "    \"\"\"\n",
    "    Convierte un subconjunto de datos recientes en un tensor compatible con el modelo.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame con los datos recientes del usuario.\n",
    "                             Debe contener columnas ['x', 'y', 'd', 't'].\n",
    "        grid_size (int): Tamaño del plano espacial (default: 200x200).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor con forma (1, N, 3, grid_size, grid_size).\n",
    "    \"\"\"\n",
    "    # Número de timeslots en los datos recientes\n",
    "    seq_len = len(data)\n",
    "\n",
    "    # Crear el tensor vacío\n",
    "    tensor = np.zeros(\n",
    "        (seq_len, 3, grid_size, grid_size)\n",
    "    )  # (N, 3, grid_size, grid_size)\n",
    "\n",
    "    # Iterar por cada timeslot en los datos recientes\n",
    "    for j, row in enumerate(data.itertuples(index=False)):\n",
    "        x, y, d, t = row.x, row.y, row.d, row.t\n",
    "\n",
    "        # Canal 1: Representación gaussiana\n",
    "        gaussian_layer = np.zeros((grid_size, grid_size))\n",
    "        gaussian_layer[x, y] = 1\n",
    "        gaussian_layer = gaussian_filter(gaussian_layer, sigma=2)\n",
    "\n",
    "        # Canal 2: Día de la semana\n",
    "        day_layer = np.zeros((grid_size, grid_size))\n",
    "        day_layer[x, y] = d % 7 + 1\n",
    "\n",
    "        # Canal 3: Timeslot\n",
    "        timeslot_layer = np.zeros((grid_size, grid_size))\n",
    "        timeslot_layer[x, y] = t\n",
    "\n",
    "        # Asignar al tensor\n",
    "        tensor[j, 0] = gaussian_layer\n",
    "        tensor[j, 1] = day_layer\n",
    "        tensor[j, 2] = timeslot_layer\n",
    "\n",
    "    # Convertir a tensor de PyTorch y añadir la dimensión batch\n",
    "    return torch.tensor(tensor, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "def generate_sequences(data, grid_size=200):\n",
    "    sequences = []\n",
    "    timeslots = []\n",
    "    coordinates = []\n",
    "    user_ids = []\n",
    "\n",
    "    for user_id, group in data.groupby(\"uid\"):\n",
    "        group = group.sort_values(by=\"t_unificado\").reset_index(drop=True)\n",
    "\n",
    "        # Generar secuencias\n",
    "        for i in range(len(group) - 5):  # Tamaño fijo de 5\n",
    "            # Crear ventana de datos recientes\n",
    "            recent_data = group.iloc[i : i + 5]  # Subconjunto de 5 filas\n",
    "\n",
    "            # Usar create_tensor_from_data para construir el tensor\n",
    "            tensor = (\n",
    "                create_tensor_from_data(recent_data, grid_size=grid_size)\n",
    "                .squeeze(0)\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "            # Target: El siguiente timeslot y coordenadas\n",
    "            t_target = group[\"t_unificado\"].iloc[i + 5]\n",
    "            coord_target = [group[\"x\"].iloc[i + 5], group[\"y\"].iloc[i + 5]]\n",
    "\n",
    "            # Guardar resultados\n",
    "            sequences.append(tensor)\n",
    "            timeslots.append(t_target)\n",
    "            coordinates.append(coord_target)\n",
    "            user_ids.append(user_id)\n",
    "\n",
    "    return (\n",
    "        np.array(sequences),\n",
    "        np.array(timeslots),\n",
    "        np.array(coordinates),\n",
    "        np.array(user_ids),\n",
    "    )\n",
    "\n",
    "\n",
    "# Generar datos\n",
    "sequences, timeslots, coordinates, user_ids = generate_sequences(df)\n",
    "# display(sequences.shape, timeslots.shape, coordinates.shape, user_ids.shape)\n",
    "# max_position = np.unravel_index(\n",
    "#     np.argmax(sequences[5][0][0], axis=None), sequences[5][0][0].shape\n",
    "# )\n",
    "# display(max_position)\n",
    "display(sequences[1][0][0].max())\n",
    "# display(sequences[1][0][0][0][41])\n",
    "# display(timeslots)\n",
    "# display(coordinates)\n",
    "# display(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, timeslots, coordinates, user_ids):\n",
    "        \"\"\"\n",
    "        sequences: Tensor con las secuencias temporales (N, 3, 200, 200)\n",
    "        timeslots: Secuencia de targets para los timeslots predichos\n",
    "        coordinates: Secuencia de targets para las coordenadas predichas (x, y)\n",
    "        user_ids: Lista de IDs de usuarios\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.timeslots = torch.tensor(timeslots, dtype=torch.float32)\n",
    "        self.coordinates = torch.tensor(coordinates, dtype=torch.float32)\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.sequences[idx],  # Tensor espacial\n",
    "            self.timeslots[idx],  # Target timeslot unificado\n",
    "            self.coordinates[idx],  # Target (x, y)\n",
    "            self.user_ids[idx],  # User ID\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 4 5\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Crear el Dataset\n",
    "dataset = LSTMDataset(sequences, timeslots, coordinates, user_ids)\n",
    "\n",
    "# Dividir el dataset en entrenamiento, validación y prueba\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(0, train_size))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))\n",
    "test_dataset = torch.utils.data.Subset(\n",
    "    dataset, range(train_size + val_size, len(dataset))\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "# Configurar los DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia Espacial: torch.Size([16, 5, 3, 200, 200])\n",
      "Timeslots Target: torch.Size([16])\n",
      "Coordenadas Target: torch.Size([16, 2])\n",
      "IDs de Usuario: torch.Size([16])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Verificar la estructura de un batch\n",
    "for batch in train_loader:\n",
    "    sequence_batch, timeslot_batch, coordinate_batch, user_id_batch = batch\n",
    "\n",
    "    # Imprimir dimensiones\n",
    "    print(\"Secuencia Espacial:\", sequence_batch.shape)  # (batch_size, 5, 3, 200, 200)\n",
    "    print(\"Timeslots Target:\", timeslot_batch.shape)  # (batch_size,)\n",
    "    print(\"Coordenadas Target:\", coordinate_batch.shape)  # (batch_size, 2)\n",
    "    print(\"IDs de Usuario:\", user_id_batch.shape)  # (batch_size,)\n",
    "    # print(\"\\nEjemplo de Secuencia Espacial (primer elemento del batch):\")\n",
    "    # print(sequence_batch[0])  # Imprime la primera secuencia\n",
    "\n",
    "    # print(\"\\nEjemplo de Timeslot Target (primer elemento del batch):\")\n",
    "    # print(timeslot_batch[0])  # Imprime el primer timeslot target\n",
    "\n",
    "    # print(\"\\nEjemplo de Coordenadas Target (primer elemento del batch):\")\n",
    "    # print(coordinate_batch[0])  # Imprime las coordenadas target (x, y)\n",
    "\n",
    "    # print(\"\\nEjemplo de ID de Usuario (primer elemento del batch):\")\n",
    "    print(user_id_batch[0])  # Imprime el ID del usuario\n",
    "    break\n",
    "    # print(\"\\n=====================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_users, embedding_dim=16, cnn_output_dim=64, lstm_hidden_dim=128\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Embedding para usuarios\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "\n",
    "        # Red convolucional (procesa los 3 canales espaciales)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 50 * 50, cnn_output_dim),\n",
    "        )\n",
    "\n",
    "        # LSTM para capturar patrones temporales\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_output_dim + embedding_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Capa para predecir el siguiente timeslot\n",
    "        self.fc_timeslot = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "        # Capa para predecir las coordenadas (x, y)\n",
    "        self.fc_coordinates = nn.Linear(lstm_hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x, user_ids):\n",
    "        batch_size, seq_len, _, _, _ = x.size()\n",
    "\n",
    "        # Procesar mapas espaciales con CNN\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            cnn_out = self.cnn(x[:, t, :, :, :])  # Procesar cada timeslot\n",
    "            cnn_features.append(cnn_out)\n",
    "\n",
    "        cnn_features = torch.stack(\n",
    "            cnn_features, dim=1\n",
    "        )  # (batch_size, seq_len, cnn_output_dim)\n",
    "\n",
    "        # Embedding de usuarios\n",
    "        user_embeds = self.user_embedding(user_ids).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenar características de CNN y embeddings\n",
    "        lstm_input = torch.cat((cnn_features, user_embeds), dim=2)\n",
    "\n",
    "        # Pasar por LSTM\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "\n",
    "        # Predicción del último timeslot\n",
    "        timeslot_pred = self.fc_timeslot(lstm_out[:, -1, :]).squeeze(-1)\n",
    "\n",
    "        # Predicción de las coordenadas (x, y)\n",
    "        coord_pred = self.fc_coordinates(lstm_out[:, -1, :])\n",
    "\n",
    "        return timeslot_pred, coord_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizado: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configurar el dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo utilizado: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(timeslot_pred, timeslot_true, coord_pred, coord_true):\n",
    "    timeslot_loss = nn.MSELoss()(timeslot_pred, timeslot_true)\n",
    "    coord_loss = nn.MSELoss()(coord_pred, coord_true)\n",
    "    return timeslot_loss + coord_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo\n",
    "num_users = len(df[\"uid\"].unique())  # Número de usuarios únicos en los datos\n",
    "model = Model(num_users=num_users).to(device)\n",
    "\n",
    "# Configurar el optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500, Loss: 74371.7754\n",
      "Epoch 2/1500, Loss: 73709.4922\n",
      "Epoch 3/1500, Loss: 73162.6895\n",
      "Epoch 4/1500, Loss: 72657.4805\n",
      "Epoch 5/1500, Loss: 72169.7637\n",
      "Epoch 6/1500, Loss: 71770.4551\n",
      "Epoch 7/1500, Loss: 71422.9043\n",
      "Epoch 8/1500, Loss: 71141.3320\n",
      "Epoch 9/1500, Loss: 70897.5469\n",
      "Epoch 10/1500, Loss: 70687.0273\n",
      "Epoch 11/1500, Loss: 70515.2832\n",
      "Epoch 12/1500, Loss: 70363.2637\n",
      "Epoch 13/1500, Loss: 70216.3828\n",
      "Epoch 14/1500, Loss: 70072.9395\n",
      "Epoch 15/1500, Loss: 69931.7266\n",
      "Epoch 16/1500, Loss: 69790.5957\n",
      "Epoch 17/1500, Loss: 69649.6836\n",
      "Epoch 18/1500, Loss: 69509.1680\n",
      "Epoch 19/1500, Loss: 69369.1523\n",
      "Epoch 20/1500, Loss: 69229.7441\n",
      "Epoch 21/1500, Loss: 69090.9824\n",
      "Epoch 22/1500, Loss: 68952.9102\n",
      "Epoch 23/1500, Loss: 68815.5508\n",
      "Epoch 24/1500, Loss: 68678.9004\n",
      "Epoch 25/1500, Loss: 68542.9922\n",
      "Epoch 26/1500, Loss: 68407.7930\n",
      "Epoch 27/1500, Loss: 68273.2969\n",
      "Epoch 28/1500, Loss: 68139.5020\n",
      "Epoch 29/1500, Loss: 68006.3906\n",
      "Epoch 30/1500, Loss: 67873.9453\n",
      "Epoch 31/1500, Loss: 67742.1465\n",
      "Epoch 32/1500, Loss: 67610.9785\n",
      "Epoch 33/1500, Loss: 67480.4336\n",
      "Epoch 34/1500, Loss: 67350.4766\n",
      "Epoch 35/1500, Loss: 67221.1133\n",
      "Epoch 36/1500, Loss: 67092.3164\n",
      "Epoch 37/1500, Loss: 66964.0684\n",
      "Epoch 38/1500, Loss: 66836.3574\n",
      "Epoch 39/1500, Loss: 66709.1680\n",
      "Epoch 40/1500, Loss: 66582.4863\n",
      "Epoch 41/1500, Loss: 66456.3086\n",
      "Epoch 42/1500, Loss: 66330.6152\n",
      "Epoch 43/1500, Loss: 66205.3906\n",
      "Epoch 44/1500, Loss: 66080.6387\n",
      "Epoch 45/1500, Loss: 65956.3262\n",
      "Epoch 46/1500, Loss: 65832.4648\n",
      "Epoch 47/1500, Loss: 65709.0371\n",
      "Epoch 48/1500, Loss: 65586.0234\n",
      "Epoch 49/1500, Loss: 65463.4375\n",
      "Epoch 50/1500, Loss: 65341.2539\n",
      "Epoch 51/1500, Loss: 65219.4727\n",
      "Epoch 52/1500, Loss: 65098.0762\n",
      "Epoch 53/1500, Loss: 64977.0879\n",
      "Epoch 54/1500, Loss: 64856.4629\n",
      "Epoch 55/1500, Loss: 64736.2129\n",
      "Epoch 56/1500, Loss: 64616.3398\n",
      "Epoch 57/1500, Loss: 64496.8203\n",
      "Epoch 58/1500, Loss: 64377.6680\n",
      "Epoch 59/1500, Loss: 64258.8594\n",
      "Epoch 60/1500, Loss: 64140.3945\n",
      "Epoch 61/1500, Loss: 64022.2812\n",
      "Epoch 62/1500, Loss: 63904.5059\n",
      "Epoch 63/1500, Loss: 63787.0488\n",
      "Epoch 64/1500, Loss: 63669.9102\n",
      "Epoch 65/1500, Loss: 63552.8613\n",
      "Epoch 66/1500, Loss: 63434.1426\n",
      "Epoch 67/1500, Loss: 63316.0957\n",
      "Epoch 68/1500, Loss: 63198.7188\n",
      "Epoch 69/1500, Loss: 63081.5039\n",
      "Epoch 70/1500, Loss: 62964.5078\n",
      "Epoch 71/1500, Loss: 62847.7676\n",
      "Epoch 72/1500, Loss: 62731.3066\n",
      "Epoch 73/1500, Loss: 62615.1523\n",
      "Epoch 74/1500, Loss: 62499.3164\n",
      "Epoch 75/1500, Loss: 62383.8027\n",
      "Epoch 76/1500, Loss: 62268.6055\n",
      "Epoch 77/1500, Loss: 62153.7305\n",
      "Epoch 78/1500, Loss: 62039.1836\n",
      "Epoch 79/1500, Loss: 61924.9512\n",
      "Epoch 80/1500, Loss: 61811.0352\n",
      "Epoch 81/1500, Loss: 61697.4355\n",
      "Epoch 82/1500, Loss: 61584.1445\n",
      "Epoch 83/1500, Loss: 61471.1680\n",
      "Epoch 84/1500, Loss: 61358.4902\n",
      "Epoch 85/1500, Loss: 61246.1172\n",
      "Epoch 86/1500, Loss: 61134.0430\n",
      "Epoch 87/1500, Loss: 61022.2617\n",
      "Epoch 88/1500, Loss: 60910.7676\n",
      "Epoch 89/1500, Loss: 60799.5625\n",
      "Epoch 90/1500, Loss: 60688.6387\n",
      "Epoch 91/1500, Loss: 60578.0039\n",
      "Epoch 92/1500, Loss: 60467.6445\n",
      "Epoch 93/1500, Loss: 60357.5645\n",
      "Epoch 94/1500, Loss: 60247.7520\n",
      "Epoch 95/1500, Loss: 60138.2090\n",
      "Epoch 96/1500, Loss: 60028.9473\n",
      "Epoch 97/1500, Loss: 59919.9355\n",
      "Epoch 98/1500, Loss: 59811.1934\n",
      "Epoch 99/1500, Loss: 59702.7109\n",
      "Epoch 100/1500, Loss: 59594.4824\n",
      "Epoch 101/1500, Loss: 59486.5195\n",
      "Epoch 102/1500, Loss: 59378.8027\n",
      "Epoch 103/1500, Loss: 59271.3398\n",
      "Epoch 104/1500, Loss: 59164.1270\n",
      "Epoch 105/1500, Loss: 59057.1602\n",
      "Epoch 106/1500, Loss: 58950.4492\n",
      "Epoch 107/1500, Loss: 58843.9688\n",
      "Epoch 108/1500, Loss: 58737.7227\n",
      "Epoch 109/1500, Loss: 58630.5195\n",
      "Epoch 110/1500, Loss: 58507.8867\n",
      "Epoch 111/1500, Loss: 58400.0098\n",
      "Epoch 112/1500, Loss: 58294.4434\n",
      "Epoch 113/1500, Loss: 58188.8867\n",
      "Epoch 114/1500, Loss: 58083.5957\n",
      "Epoch 115/1500, Loss: 57978.5293\n",
      "Epoch 116/1500, Loss: 57873.6836\n",
      "Epoch 117/1500, Loss: 57769.0547\n",
      "Epoch 118/1500, Loss: 57664.6387\n",
      "Epoch 119/1500, Loss: 57560.4590\n",
      "Epoch 120/1500, Loss: 57456.4922\n",
      "Epoch 121/1500, Loss: 57352.7578\n",
      "Epoch 122/1500, Loss: 57249.2578\n",
      "Epoch 123/1500, Loss: 57145.9902\n",
      "Epoch 124/1500, Loss: 57042.9434\n",
      "Epoch 125/1500, Loss: 56940.1348\n",
      "Epoch 126/1500, Loss: 56837.5430\n",
      "Epoch 127/1500, Loss: 56735.1777\n",
      "Epoch 128/1500, Loss: 56633.0488\n",
      "Epoch 129/1500, Loss: 56531.1328\n",
      "Epoch 130/1500, Loss: 56429.4395\n",
      "Epoch 131/1500, Loss: 56327.9609\n",
      "Epoch 132/1500, Loss: 56226.7070\n",
      "Epoch 133/1500, Loss: 56125.6680\n",
      "Epoch 134/1500, Loss: 56024.8613\n",
      "Epoch 135/1500, Loss: 55924.2539\n",
      "Epoch 136/1500, Loss: 55823.8691\n",
      "Epoch 137/1500, Loss: 55723.6895\n",
      "Epoch 138/1500, Loss: 55623.7305\n",
      "Epoch 139/1500, Loss: 55523.9844\n",
      "Epoch 140/1500, Loss: 55424.4531\n",
      "Epoch 141/1500, Loss: 55325.1172\n",
      "Epoch 142/1500, Loss: 55225.9961\n",
      "Epoch 143/1500, Loss: 55127.0840\n",
      "Epoch 144/1500, Loss: 55028.3730\n",
      "Epoch 145/1500, Loss: 54929.8711\n",
      "Epoch 146/1500, Loss: 54831.5762\n",
      "Epoch 147/1500, Loss: 54733.4863\n",
      "Epoch 148/1500, Loss: 54635.5938\n",
      "Epoch 149/1500, Loss: 54537.9014\n",
      "Epoch 150/1500, Loss: 54440.4141\n",
      "Epoch 151/1500, Loss: 54343.1260\n",
      "Epoch 152/1500, Loss: 54246.0371\n",
      "Epoch 153/1500, Loss: 54149.1426\n",
      "Epoch 154/1500, Loss: 54052.4512\n",
      "Epoch 155/1500, Loss: 53955.9551\n",
      "Epoch 156/1500, Loss: 53859.6562\n",
      "Epoch 157/1500, Loss: 53763.5557\n",
      "Epoch 158/1500, Loss: 53667.6436\n",
      "Epoch 159/1500, Loss: 53571.9219\n",
      "Epoch 160/1500, Loss: 53476.3965\n",
      "Epoch 161/1500, Loss: 53381.0615\n",
      "Epoch 162/1500, Loss: 53285.9238\n",
      "Epoch 163/1500, Loss: 53190.9746\n",
      "Epoch 164/1500, Loss: 53096.2109\n",
      "Epoch 165/1500, Loss: 53001.6426\n",
      "Epoch 166/1500, Loss: 52907.2520\n",
      "Epoch 167/1500, Loss: 52813.0586\n",
      "Epoch 168/1500, Loss: 52719.0508\n",
      "Epoch 169/1500, Loss: 52625.2246\n",
      "Epoch 170/1500, Loss: 52531.5869\n",
      "Epoch 171/1500, Loss: 52438.1387\n",
      "Epoch 172/1500, Loss: 52344.8623\n",
      "Epoch 173/1500, Loss: 52251.7842\n",
      "Epoch 174/1500, Loss: 52158.8857\n",
      "Epoch 175/1500, Loss: 52066.1650\n",
      "Epoch 176/1500, Loss: 51973.6270\n",
      "Epoch 177/1500, Loss: 51881.2783\n",
      "Epoch 178/1500, Loss: 51789.0996\n",
      "Epoch 179/1500, Loss: 51697.1064\n",
      "Epoch 180/1500, Loss: 51605.2930\n",
      "Epoch 181/1500, Loss: 51513.6572\n",
      "Epoch 182/1500, Loss: 51422.2002\n",
      "Epoch 183/1500, Loss: 51330.9189\n",
      "Epoch 184/1500, Loss: 51239.8193\n",
      "Epoch 185/1500, Loss: 51148.8926\n",
      "Epoch 186/1500, Loss: 51058.1475\n",
      "Epoch 187/1500, Loss: 50967.5693\n",
      "Epoch 188/1500, Loss: 50877.1719\n",
      "Epoch 189/1500, Loss: 50786.9502\n",
      "Epoch 190/1500, Loss: 50696.8955\n",
      "Epoch 191/1500, Loss: 50607.0156\n",
      "Epoch 192/1500, Loss: 50517.3086\n",
      "Epoch 193/1500, Loss: 50427.7773\n",
      "Epoch 194/1500, Loss: 50338.4189\n",
      "Epoch 195/1500, Loss: 50249.2266\n",
      "Epoch 196/1500, Loss: 50160.2100\n",
      "Epoch 197/1500, Loss: 50071.3613\n",
      "Epoch 198/1500, Loss: 49982.6836\n",
      "Epoch 199/1500, Loss: 49894.1719\n",
      "Epoch 200/1500, Loss: 49805.8340\n",
      "Epoch 201/1500, Loss: 49717.6660\n",
      "Epoch 202/1500, Loss: 49629.6582\n",
      "Epoch 203/1500, Loss: 49541.8223\n",
      "Epoch 204/1500, Loss: 49454.1484\n",
      "Epoch 205/1500, Loss: 49366.6465\n",
      "Epoch 206/1500, Loss: 49279.3105\n",
      "Epoch 207/1500, Loss: 49192.1328\n",
      "Epoch 208/1500, Loss: 49105.1270\n",
      "Epoch 209/1500, Loss: 49018.2881\n",
      "Epoch 210/1500, Loss: 48931.6084\n",
      "Epoch 211/1500, Loss: 48845.0977\n",
      "Epoch 212/1500, Loss: 48758.7432\n",
      "Epoch 213/1500, Loss: 48672.5488\n",
      "Epoch 214/1500, Loss: 48586.5195\n",
      "Epoch 215/1500, Loss: 48500.6602\n",
      "Epoch 216/1500, Loss: 48414.9551\n",
      "Epoch 217/1500, Loss: 48329.4082\n",
      "Epoch 218/1500, Loss: 48244.0205\n",
      "Epoch 219/1500, Loss: 48158.8047\n",
      "Epoch 220/1500, Loss: 48073.7393\n",
      "Epoch 221/1500, Loss: 47988.8389\n",
      "Epoch 222/1500, Loss: 47904.0938\n",
      "Epoch 223/1500, Loss: 47819.5029\n",
      "Epoch 224/1500, Loss: 47735.0752\n",
      "Epoch 225/1500, Loss: 47650.8018\n",
      "Epoch 226/1500, Loss: 47566.6895\n",
      "Epoch 227/1500, Loss: 47482.7363\n",
      "Epoch 228/1500, Loss: 47398.9307\n",
      "Epoch 229/1500, Loss: 47315.2871\n",
      "Epoch 230/1500, Loss: 47231.7969\n",
      "Epoch 231/1500, Loss: 47148.4639\n",
      "Epoch 232/1500, Loss: 47065.2803\n",
      "Epoch 233/1500, Loss: 46982.2588\n",
      "Epoch 234/1500, Loss: 46899.3809\n",
      "Epoch 235/1500, Loss: 46816.6689\n",
      "Epoch 236/1500, Loss: 46734.0996\n",
      "Epoch 237/1500, Loss: 46651.6846\n",
      "Epoch 238/1500, Loss: 46569.4219\n",
      "Epoch 239/1500, Loss: 46487.3174\n",
      "Epoch 240/1500, Loss: 46405.3643\n",
      "Epoch 241/1500, Loss: 46323.5605\n",
      "Epoch 242/1500, Loss: 46241.9053\n",
      "Epoch 243/1500, Loss: 46160.3965\n",
      "Epoch 244/1500, Loss: 46079.0488\n",
      "Epoch 245/1500, Loss: 45997.8418\n",
      "Epoch 246/1500, Loss: 45916.7852\n",
      "Epoch 247/1500, Loss: 45835.8770\n",
      "Epoch 248/1500, Loss: 45755.1172\n",
      "Epoch 249/1500, Loss: 45674.5137\n",
      "Epoch 250/1500, Loss: 45594.0508\n",
      "Epoch 251/1500, Loss: 45513.7334\n",
      "Epoch 252/1500, Loss: 45433.5645\n",
      "Epoch 253/1500, Loss: 45353.5479\n",
      "Epoch 254/1500, Loss: 45273.6758\n",
      "Epoch 255/1500, Loss: 45193.9521\n",
      "Epoch 256/1500, Loss: 45114.3652\n",
      "Epoch 257/1500, Loss: 45034.9316\n",
      "Epoch 258/1500, Loss: 44955.6396\n",
      "Epoch 259/1500, Loss: 44876.4912\n",
      "Epoch 260/1500, Loss: 44797.4912\n",
      "Epoch 261/1500, Loss: 44718.6328\n",
      "Epoch 262/1500, Loss: 44639.9121\n",
      "Epoch 263/1500, Loss: 44561.3418\n",
      "Epoch 264/1500, Loss: 44482.9160\n",
      "Epoch 265/1500, Loss: 44404.6309\n",
      "Epoch 266/1500, Loss: 44326.4873\n",
      "Epoch 267/1500, Loss: 44248.4863\n",
      "Epoch 268/1500, Loss: 44170.6289\n",
      "Epoch 269/1500, Loss: 44092.9072\n",
      "Epoch 270/1500, Loss: 44015.3291\n",
      "Epoch 271/1500, Loss: 43937.8926\n",
      "Epoch 272/1500, Loss: 43860.5977\n",
      "Epoch 273/1500, Loss: 43783.4434\n",
      "Epoch 274/1500, Loss: 43706.4238\n",
      "Epoch 275/1500, Loss: 43629.5508\n",
      "Epoch 276/1500, Loss: 43552.8096\n",
      "Epoch 277/1500, Loss: 43476.2090\n",
      "Epoch 278/1500, Loss: 43399.7510\n",
      "Epoch 279/1500, Loss: 43323.4277\n",
      "Epoch 280/1500, Loss: 43247.2412\n",
      "Epoch 281/1500, Loss: 43171.1973\n",
      "Epoch 282/1500, Loss: 43095.2871\n",
      "Epoch 283/1500, Loss: 43019.5166\n",
      "Epoch 284/1500, Loss: 42943.8828\n",
      "Epoch 285/1500, Loss: 42868.3770\n",
      "Epoch 286/1500, Loss: 42793.0176\n",
      "Epoch 287/1500, Loss: 42717.7852\n",
      "Epoch 288/1500, Loss: 42642.6953\n",
      "Epoch 289/1500, Loss: 42567.7363\n",
      "Epoch 290/1500, Loss: 42492.9121\n",
      "Epoch 291/1500, Loss: 42418.2314\n",
      "Epoch 292/1500, Loss: 42343.6699\n",
      "Epoch 293/1500, Loss: 42269.2520\n",
      "Epoch 294/1500, Loss: 42194.9658\n",
      "Epoch 295/1500, Loss: 42120.8164\n",
      "Epoch 296/1500, Loss: 42046.7939\n",
      "Epoch 297/1500, Loss: 41972.9053\n",
      "Epoch 298/1500, Loss: 41899.1543\n",
      "Epoch 299/1500, Loss: 41825.5352\n",
      "Epoch 300/1500, Loss: 41752.0391\n",
      "Epoch 301/1500, Loss: 41678.6816\n",
      "Epoch 302/1500, Loss: 41605.4541\n",
      "Epoch 303/1500, Loss: 41532.3574\n",
      "Epoch 304/1500, Loss: 41459.3906\n",
      "Epoch 305/1500, Loss: 41386.5586\n",
      "Epoch 306/1500, Loss: 41313.8516\n",
      "Epoch 307/1500, Loss: 41241.2793\n",
      "Epoch 308/1500, Loss: 41168.8350\n",
      "Epoch 309/1500, Loss: 41096.5205\n",
      "Epoch 310/1500, Loss: 41024.3350\n",
      "Epoch 311/1500, Loss: 40952.2734\n",
      "Epoch 312/1500, Loss: 40880.3486\n",
      "Epoch 313/1500, Loss: 40808.5518\n",
      "Epoch 314/1500, Loss: 40736.8799\n",
      "Epoch 315/1500, Loss: 40665.3320\n",
      "Epoch 316/1500, Loss: 40593.9160\n",
      "Epoch 317/1500, Loss: 40522.6279\n",
      "Epoch 318/1500, Loss: 40451.4648\n",
      "Epoch 319/1500, Loss: 40380.4316\n",
      "Epoch 320/1500, Loss: 40309.5186\n",
      "Epoch 321/1500, Loss: 40238.7402\n",
      "Epoch 322/1500, Loss: 40168.0869\n",
      "Epoch 323/1500, Loss: 40097.5518\n",
      "Epoch 324/1500, Loss: 40027.1504\n",
      "Epoch 325/1500, Loss: 39956.8691\n",
      "Epoch 326/1500, Loss: 39886.7178\n",
      "Epoch 327/1500, Loss: 39816.6855\n",
      "Epoch 328/1500, Loss: 39746.7812\n",
      "Epoch 329/1500, Loss: 39677.0049\n",
      "Epoch 330/1500, Loss: 39607.3428\n",
      "Epoch 331/1500, Loss: 39537.8203\n",
      "Epoch 332/1500, Loss: 39468.4102\n",
      "Epoch 333/1500, Loss: 39399.1230\n",
      "Epoch 334/1500, Loss: 39329.9600\n",
      "Epoch 335/1500, Loss: 39260.9258\n",
      "Epoch 336/1500, Loss: 39192.0029\n",
      "Epoch 337/1500, Loss: 39123.2109\n",
      "Epoch 338/1500, Loss: 39054.5361\n",
      "Epoch 339/1500, Loss: 38985.9941\n",
      "Epoch 340/1500, Loss: 38917.5645\n",
      "Epoch 341/1500, Loss: 38849.2510\n",
      "Epoch 342/1500, Loss: 38781.0684\n",
      "Epoch 343/1500, Loss: 38713.0107\n",
      "Epoch 344/1500, Loss: 38645.0635\n",
      "Epoch 345/1500, Loss: 38577.2412\n",
      "Epoch 346/1500, Loss: 38509.5391\n",
      "Epoch 347/1500, Loss: 38441.9521\n",
      "Epoch 348/1500, Loss: 38374.4941\n",
      "Epoch 349/1500, Loss: 38307.1465\n",
      "Epoch 350/1500, Loss: 38239.9307\n",
      "Epoch 351/1500, Loss: 38172.8213\n",
      "Epoch 352/1500, Loss: 38105.8428\n",
      "Epoch 353/1500, Loss: 38038.9727\n",
      "Epoch 354/1500, Loss: 37972.2285\n",
      "Epoch 355/1500, Loss: 37905.6006\n",
      "Epoch 356/1500, Loss: 37839.0859\n",
      "Epoch 357/1500, Loss: 37772.6943\n",
      "Epoch 358/1500, Loss: 37706.4189\n",
      "Epoch 359/1500, Loss: 37640.2588\n",
      "Epoch 360/1500, Loss: 37574.2236\n",
      "Epoch 361/1500, Loss: 37508.2988\n",
      "Epoch 362/1500, Loss: 37442.4912\n",
      "Epoch 363/1500, Loss: 37376.8008\n",
      "Epoch 364/1500, Loss: 37311.2295\n",
      "Epoch 365/1500, Loss: 37245.7744\n",
      "Epoch 366/1500, Loss: 37180.4316\n",
      "Epoch 367/1500, Loss: 37115.2041\n",
      "Epoch 368/1500, Loss: 37050.1006\n",
      "Epoch 369/1500, Loss: 36985.1045\n",
      "Epoch 370/1500, Loss: 36920.2266\n",
      "Epoch 371/1500, Loss: 36855.4648\n",
      "Epoch 372/1500, Loss: 36790.8145\n",
      "Epoch 373/1500, Loss: 36726.2773\n",
      "Epoch 374/1500, Loss: 36661.8623\n",
      "Epoch 375/1500, Loss: 36597.5557\n",
      "Epoch 376/1500, Loss: 36533.3672\n",
      "Epoch 377/1500, Loss: 36469.2861\n",
      "Epoch 378/1500, Loss: 36405.3252\n",
      "Epoch 379/1500, Loss: 36341.4717\n",
      "Epoch 380/1500, Loss: 36277.7354\n",
      "Epoch 381/1500, Loss: 36214.1152\n",
      "Epoch 382/1500, Loss: 36150.6045\n",
      "Epoch 383/1500, Loss: 36087.2051\n",
      "Epoch 384/1500, Loss: 36023.9209\n",
      "Epoch 385/1500, Loss: 35960.7422\n",
      "Epoch 386/1500, Loss: 35897.6836\n",
      "Epoch 387/1500, Loss: 35834.7402\n",
      "Epoch 388/1500, Loss: 35771.8965\n",
      "Epoch 389/1500, Loss: 35709.1719\n",
      "Epoch 390/1500, Loss: 35646.5547\n",
      "Epoch 391/1500, Loss: 35584.0586\n",
      "Epoch 392/1500, Loss: 35521.6621\n",
      "Epoch 393/1500, Loss: 35459.3877\n",
      "Epoch 394/1500, Loss: 35397.2168\n",
      "Epoch 395/1500, Loss: 35335.1553\n",
      "Epoch 396/1500, Loss: 35273.2021\n",
      "Epoch 397/1500, Loss: 35211.3633\n",
      "Epoch 398/1500, Loss: 35149.6367\n",
      "Epoch 399/1500, Loss: 35088.0127\n",
      "Epoch 400/1500, Loss: 35026.5049\n",
      "Epoch 401/1500, Loss: 34965.1069\n",
      "Epoch 402/1500, Loss: 34903.8096\n",
      "Epoch 403/1500, Loss: 34842.6309\n",
      "Epoch 404/1500, Loss: 34781.5605\n",
      "Epoch 405/1500, Loss: 34720.5908\n",
      "Epoch 406/1500, Loss: 34659.7393\n",
      "Epoch 407/1500, Loss: 34598.9854\n",
      "Epoch 408/1500, Loss: 34538.3457\n",
      "Epoch 409/1500, Loss: 34477.8179\n",
      "Epoch 410/1500, Loss: 34417.3950\n",
      "Epoch 411/1500, Loss: 34357.0776\n",
      "Epoch 412/1500, Loss: 34296.8677\n",
      "Epoch 413/1500, Loss: 34236.7681\n",
      "Epoch 414/1500, Loss: 34176.7720\n",
      "Epoch 415/1500, Loss: 34116.8853\n",
      "Epoch 416/1500, Loss: 34057.1099\n",
      "Epoch 417/1500, Loss: 33997.4346\n",
      "Epoch 418/1500, Loss: 33937.8677\n",
      "Epoch 419/1500, Loss: 33878.4087\n",
      "Epoch 420/1500, Loss: 33819.0518\n",
      "Epoch 421/1500, Loss: 33759.8037\n",
      "Epoch 422/1500, Loss: 33700.6650\n",
      "Epoch 423/1500, Loss: 33641.6255\n",
      "Epoch 424/1500, Loss: 33582.6919\n",
      "Epoch 425/1500, Loss: 33523.8716\n",
      "Epoch 426/1500, Loss: 33465.1470\n",
      "Epoch 427/1500, Loss: 33406.5312\n",
      "Epoch 428/1500, Loss: 33348.0166\n",
      "Epoch 429/1500, Loss: 33289.6191\n",
      "Epoch 430/1500, Loss: 33231.3179\n",
      "Epoch 431/1500, Loss: 33173.1196\n",
      "Epoch 432/1500, Loss: 33115.0283\n",
      "Epoch 433/1500, Loss: 33057.0352\n",
      "Epoch 434/1500, Loss: 32999.1499\n",
      "Epoch 435/1500, Loss: 32941.3774\n",
      "Epoch 436/1500, Loss: 32883.6958\n",
      "Epoch 437/1500, Loss: 32826.1206\n",
      "Epoch 438/1500, Loss: 32768.6538\n",
      "Epoch 439/1500, Loss: 32711.2881\n",
      "Epoch 440/1500, Loss: 32654.0229\n",
      "Epoch 441/1500, Loss: 32596.8643\n",
      "Epoch 442/1500, Loss: 32539.7993\n",
      "Epoch 443/1500, Loss: 32482.8472\n",
      "Epoch 444/1500, Loss: 32425.9966\n",
      "Epoch 445/1500, Loss: 32369.2437\n",
      "Epoch 446/1500, Loss: 32312.5938\n",
      "Epoch 447/1500, Loss: 32256.0488\n",
      "Epoch 448/1500, Loss: 32199.6050\n",
      "Epoch 449/1500, Loss: 32143.2607\n",
      "Epoch 450/1500, Loss: 32087.0190\n",
      "Epoch 451/1500, Loss: 32030.8813\n",
      "Epoch 452/1500, Loss: 31974.8359\n",
      "Epoch 453/1500, Loss: 31918.9033\n",
      "Epoch 454/1500, Loss: 31863.0688\n",
      "Epoch 455/1500, Loss: 31807.3311\n",
      "Epoch 456/1500, Loss: 31751.6934\n",
      "Epoch 457/1500, Loss: 31696.1616\n",
      "Epoch 458/1500, Loss: 31640.7275\n",
      "Epoch 459/1500, Loss: 31585.3950\n",
      "Epoch 460/1500, Loss: 31530.1572\n",
      "Epoch 461/1500, Loss: 31475.0239\n",
      "Epoch 462/1500, Loss: 31419.9917\n",
      "Epoch 463/1500, Loss: 31365.0542\n",
      "Epoch 464/1500, Loss: 31310.2241\n",
      "Epoch 465/1500, Loss: 31255.4849\n",
      "Epoch 466/1500, Loss: 31200.8496\n",
      "Epoch 467/1500, Loss: 31146.3135\n",
      "Epoch 468/1500, Loss: 31091.8730\n",
      "Epoch 469/1500, Loss: 31037.5342\n",
      "Epoch 470/1500, Loss: 30983.2979\n",
      "Epoch 471/1500, Loss: 30929.1519\n",
      "Epoch 472/1500, Loss: 30875.1099\n",
      "Epoch 473/1500, Loss: 30821.1636\n",
      "Epoch 474/1500, Loss: 30767.3198\n",
      "Epoch 475/1500, Loss: 30713.5669\n",
      "Epoch 476/1500, Loss: 30659.9180\n",
      "Epoch 477/1500, Loss: 30606.3667\n",
      "Epoch 478/1500, Loss: 30552.9077\n",
      "Epoch 479/1500, Loss: 30499.5557\n",
      "Epoch 480/1500, Loss: 30446.2896\n",
      "Epoch 481/1500, Loss: 30393.1294\n",
      "Epoch 482/1500, Loss: 30340.0635\n",
      "Epoch 483/1500, Loss: 30287.0957\n",
      "Epoch 484/1500, Loss: 30234.2197\n",
      "Epoch 485/1500, Loss: 30181.4482\n",
      "Epoch 486/1500, Loss: 30128.7671\n",
      "Epoch 487/1500, Loss: 30076.1855\n",
      "Epoch 488/1500, Loss: 30023.6948\n",
      "Epoch 489/1500, Loss: 29971.3081\n",
      "Epoch 490/1500, Loss: 29919.0132\n",
      "Epoch 491/1500, Loss: 29866.8159\n",
      "Epoch 492/1500, Loss: 29814.7144\n",
      "Epoch 493/1500, Loss: 29762.7031\n",
      "Epoch 494/1500, Loss: 29710.7925\n",
      "Epoch 495/1500, Loss: 29658.9771\n",
      "Epoch 496/1500, Loss: 29607.2534\n",
      "Epoch 497/1500, Loss: 29555.6294\n",
      "Epoch 498/1500, Loss: 29504.0981\n",
      "Epoch 499/1500, Loss: 29452.6636\n",
      "Epoch 500/1500, Loss: 29401.3237\n",
      "Epoch 501/1500, Loss: 29350.0757\n",
      "Epoch 502/1500, Loss: 29298.9263\n",
      "Epoch 503/1500, Loss: 29247.8643\n",
      "Epoch 504/1500, Loss: 29196.9019\n",
      "Epoch 505/1500, Loss: 29146.0337\n",
      "Epoch 506/1500, Loss: 29095.2646\n",
      "Epoch 507/1500, Loss: 29044.5762\n",
      "Epoch 508/1500, Loss: 28993.9878\n",
      "Epoch 509/1500, Loss: 28943.4951\n",
      "Epoch 510/1500, Loss: 28893.0952\n",
      "Epoch 511/1500, Loss: 28842.7891\n",
      "Epoch 512/1500, Loss: 28792.5771\n",
      "Epoch 513/1500, Loss: 28742.4517\n",
      "Epoch 514/1500, Loss: 28692.4287\n",
      "Epoch 515/1500, Loss: 28642.4912\n",
      "Epoch 516/1500, Loss: 28592.6470\n",
      "Epoch 517/1500, Loss: 28542.9023\n",
      "Epoch 518/1500, Loss: 28493.2451\n",
      "Epoch 519/1500, Loss: 28443.6772\n",
      "Epoch 520/1500, Loss: 28394.2056\n",
      "Epoch 521/1500, Loss: 28344.8271\n",
      "Epoch 522/1500, Loss: 28295.5396\n",
      "Epoch 523/1500, Loss: 28246.3418\n",
      "Epoch 524/1500, Loss: 28197.2402\n",
      "Epoch 525/1500, Loss: 28148.2246\n",
      "Epoch 526/1500, Loss: 28099.3057\n",
      "Epoch 527/1500, Loss: 28050.4717\n",
      "Epoch 528/1500, Loss: 28001.7368\n",
      "Epoch 529/1500, Loss: 27953.0864\n",
      "Epoch 530/1500, Loss: 27904.5317\n",
      "Epoch 531/1500, Loss: 27856.0630\n",
      "Epoch 532/1500, Loss: 27807.6899\n",
      "Epoch 533/1500, Loss: 27759.4048\n",
      "Epoch 534/1500, Loss: 27711.2119\n",
      "Epoch 535/1500, Loss: 27663.1118\n",
      "Epoch 536/1500, Loss: 27615.1011\n",
      "Epoch 537/1500, Loss: 27567.1782\n",
      "Epoch 538/1500, Loss: 27519.3447\n",
      "Epoch 539/1500, Loss: 27471.6016\n",
      "Epoch 540/1500, Loss: 27423.9487\n",
      "Epoch 541/1500, Loss: 27376.3848\n",
      "Epoch 542/1500, Loss: 27328.9131\n",
      "Epoch 543/1500, Loss: 27281.5273\n",
      "Epoch 544/1500, Loss: 27234.2354\n",
      "Epoch 545/1500, Loss: 27187.0298\n",
      "Epoch 546/1500, Loss: 27139.9165\n",
      "Epoch 547/1500, Loss: 27092.8877\n",
      "Epoch 548/1500, Loss: 27045.9502\n",
      "Epoch 549/1500, Loss: 26999.0991\n",
      "Epoch 550/1500, Loss: 26952.3394\n",
      "Epoch 551/1500, Loss: 26905.6709\n",
      "Epoch 552/1500, Loss: 26859.0825\n",
      "Epoch 553/1500, Loss: 26812.5894\n",
      "Epoch 554/1500, Loss: 26766.1846\n",
      "Epoch 555/1500, Loss: 26719.8677\n",
      "Epoch 556/1500, Loss: 26673.6343\n",
      "Epoch 557/1500, Loss: 26627.4946\n",
      "Epoch 558/1500, Loss: 26581.4336\n",
      "Epoch 559/1500, Loss: 26535.4712\n",
      "Epoch 560/1500, Loss: 26489.5879\n",
      "Epoch 561/1500, Loss: 26443.8003\n",
      "Epoch 562/1500, Loss: 26398.0918\n",
      "Epoch 563/1500, Loss: 26352.4785\n",
      "Epoch 564/1500, Loss: 26306.9502\n",
      "Epoch 565/1500, Loss: 26261.5029\n",
      "Epoch 566/1500, Loss: 26216.1470\n",
      "Epoch 567/1500, Loss: 26170.8784\n",
      "Epoch 568/1500, Loss: 26125.6929\n",
      "Epoch 569/1500, Loss: 26080.5996\n",
      "Epoch 570/1500, Loss: 26035.5869\n",
      "Epoch 571/1500, Loss: 25990.6611\n",
      "Epoch 572/1500, Loss: 25945.8242\n",
      "Epoch 573/1500, Loss: 25901.0737\n",
      "Epoch 574/1500, Loss: 25856.4102\n",
      "Epoch 575/1500, Loss: 25811.8306\n",
      "Epoch 576/1500, Loss: 25767.3374\n",
      "Epoch 577/1500, Loss: 25722.9307\n",
      "Epoch 578/1500, Loss: 25678.6079\n",
      "Epoch 579/1500, Loss: 25634.3691\n",
      "Epoch 580/1500, Loss: 25590.2153\n",
      "Epoch 581/1500, Loss: 25546.1514\n",
      "Epoch 582/1500, Loss: 25502.1641\n",
      "Epoch 583/1500, Loss: 25458.2734\n",
      "Epoch 584/1500, Loss: 25414.4585\n",
      "Epoch 585/1500, Loss: 25370.7373\n",
      "Epoch 586/1500, Loss: 25327.0952\n",
      "Epoch 587/1500, Loss: 25283.5342\n",
      "Epoch 588/1500, Loss: 25240.0591\n",
      "Epoch 589/1500, Loss: 25196.6680\n",
      "Epoch 590/1500, Loss: 25153.3647\n",
      "Epoch 591/1500, Loss: 25110.1440\n",
      "Epoch 592/1500, Loss: 25067.0083\n",
      "Epoch 593/1500, Loss: 25023.9551\n",
      "Epoch 594/1500, Loss: 24980.9854\n",
      "Epoch 595/1500, Loss: 24938.0981\n",
      "Epoch 596/1500, Loss: 24895.2939\n",
      "Epoch 597/1500, Loss: 24852.5796\n",
      "Epoch 598/1500, Loss: 24809.9438\n",
      "Epoch 599/1500, Loss: 24767.3882\n",
      "Epoch 600/1500, Loss: 24724.9209\n",
      "Epoch 601/1500, Loss: 24682.5391\n",
      "Epoch 602/1500, Loss: 24640.2300\n",
      "Epoch 603/1500, Loss: 24598.0132\n",
      "Epoch 604/1500, Loss: 24555.8745\n",
      "Epoch 605/1500, Loss: 24513.8208\n",
      "Epoch 606/1500, Loss: 24471.8477\n",
      "Epoch 607/1500, Loss: 24429.9609\n",
      "Epoch 608/1500, Loss: 24388.1504\n",
      "Epoch 609/1500, Loss: 24346.4219\n",
      "Epoch 610/1500, Loss: 24304.7812\n",
      "Epoch 611/1500, Loss: 24263.2222\n",
      "Epoch 612/1500, Loss: 24221.7407\n",
      "Epoch 613/1500, Loss: 24180.3442\n",
      "Epoch 614/1500, Loss: 24139.0264\n",
      "Epoch 615/1500, Loss: 24097.7910\n",
      "Epoch 616/1500, Loss: 24056.6357\n",
      "Epoch 617/1500, Loss: 24015.5630\n",
      "Epoch 618/1500, Loss: 23974.5757\n",
      "Epoch 619/1500, Loss: 23933.6602\n",
      "Epoch 620/1500, Loss: 23892.8340\n",
      "Epoch 621/1500, Loss: 23852.0876\n",
      "Epoch 622/1500, Loss: 23811.4209\n",
      "Epoch 623/1500, Loss: 23770.8335\n",
      "Epoch 624/1500, Loss: 23730.3254\n",
      "Epoch 625/1500, Loss: 23689.9016\n",
      "Epoch 626/1500, Loss: 23649.5562\n",
      "Epoch 627/1500, Loss: 23609.2910\n",
      "Epoch 628/1500, Loss: 23569.1069\n",
      "Epoch 629/1500, Loss: 23528.9993\n",
      "Epoch 630/1500, Loss: 23488.9766\n",
      "Epoch 631/1500, Loss: 23449.0293\n",
      "Epoch 632/1500, Loss: 23409.1675\n",
      "Epoch 633/1500, Loss: 23369.3806\n",
      "Epoch 634/1500, Loss: 23329.6760\n",
      "Epoch 635/1500, Loss: 23290.0452\n",
      "Epoch 636/1500, Loss: 23250.4971\n",
      "Epoch 637/1500, Loss: 23211.0317\n",
      "Epoch 638/1500, Loss: 23171.6431\n",
      "Epoch 639/1500, Loss: 23132.3313\n",
      "Epoch 640/1500, Loss: 23093.1001\n",
      "Epoch 641/1500, Loss: 23053.9495\n",
      "Epoch 642/1500, Loss: 23014.8748\n",
      "Epoch 643/1500, Loss: 22975.8777\n",
      "Epoch 644/1500, Loss: 22936.9609\n",
      "Epoch 645/1500, Loss: 22898.1267\n",
      "Epoch 646/1500, Loss: 22859.3652\n",
      "Epoch 647/1500, Loss: 22820.6816\n",
      "Epoch 648/1500, Loss: 22782.0793\n",
      "Epoch 649/1500, Loss: 22743.5510\n",
      "Epoch 650/1500, Loss: 22705.1052\n",
      "Epoch 651/1500, Loss: 22666.7324\n",
      "Epoch 652/1500, Loss: 22628.4451\n",
      "Epoch 653/1500, Loss: 22590.2261\n",
      "Epoch 654/1500, Loss: 22552.0913\n",
      "Epoch 655/1500, Loss: 22514.0249\n",
      "Epoch 656/1500, Loss: 22476.0432\n",
      "Epoch 657/1500, Loss: 22438.1382\n",
      "Epoch 658/1500, Loss: 22400.3120\n",
      "Epoch 659/1500, Loss: 22362.5557\n",
      "Epoch 660/1500, Loss: 22324.8772\n",
      "Epoch 661/1500, Loss: 22287.2798\n",
      "Epoch 662/1500, Loss: 22249.7581\n",
      "Epoch 663/1500, Loss: 22212.3184\n",
      "Epoch 664/1500, Loss: 22174.9482\n",
      "Epoch 665/1500, Loss: 22137.6533\n",
      "Epoch 666/1500, Loss: 22100.4375\n",
      "Epoch 667/1500, Loss: 22063.2998\n",
      "Epoch 668/1500, Loss: 22026.2371\n",
      "Epoch 669/1500, Loss: 21989.2473\n",
      "Epoch 670/1500, Loss: 21952.3315\n",
      "Epoch 671/1500, Loss: 21915.4941\n",
      "Epoch 672/1500, Loss: 21878.7334\n",
      "Epoch 673/1500, Loss: 21842.0454\n",
      "Epoch 674/1500, Loss: 21805.4368\n",
      "Epoch 675/1500, Loss: 21768.9028\n",
      "Epoch 676/1500, Loss: 21732.4419\n",
      "Epoch 677/1500, Loss: 21696.0569\n",
      "Epoch 678/1500, Loss: 21659.7490\n",
      "Epoch 679/1500, Loss: 21623.5125\n",
      "Epoch 680/1500, Loss: 21587.3535\n",
      "Epoch 681/1500, Loss: 21551.2671\n",
      "Epoch 682/1500, Loss: 21515.2559\n",
      "Epoch 683/1500, Loss: 21479.3179\n",
      "Epoch 684/1500, Loss: 21443.4573\n",
      "Epoch 685/1500, Loss: 21407.6650\n",
      "Epoch 686/1500, Loss: 21371.9539\n",
      "Epoch 687/1500, Loss: 21336.3188\n",
      "Epoch 688/1500, Loss: 21300.7549\n",
      "Epoch 689/1500, Loss: 21265.2571\n",
      "Epoch 690/1500, Loss: 21229.8440\n",
      "Epoch 691/1500, Loss: 21194.4983\n",
      "Epoch 692/1500, Loss: 21159.2273\n",
      "Epoch 693/1500, Loss: 21124.0317\n",
      "Epoch 694/1500, Loss: 21088.9089\n",
      "Epoch 695/1500, Loss: 21053.8608\n",
      "Epoch 696/1500, Loss: 21018.8801\n",
      "Epoch 697/1500, Loss: 20983.9785\n",
      "Epoch 698/1500, Loss: 20949.1475\n",
      "Epoch 699/1500, Loss: 20914.3860\n",
      "Epoch 700/1500, Loss: 20879.6997\n",
      "Epoch 701/1500, Loss: 20845.0898\n",
      "Epoch 702/1500, Loss: 20810.5493\n",
      "Epoch 703/1500, Loss: 20776.0835\n",
      "Epoch 704/1500, Loss: 20741.6833\n",
      "Epoch 705/1500, Loss: 20707.3633\n",
      "Epoch 706/1500, Loss: 20673.1155\n",
      "Epoch 707/1500, Loss: 20638.9368\n",
      "Epoch 708/1500, Loss: 20604.8284\n",
      "Epoch 709/1500, Loss: 20570.7920\n",
      "Epoch 710/1500, Loss: 20536.8315\n",
      "Epoch 711/1500, Loss: 20502.9429\n",
      "Epoch 712/1500, Loss: 20469.1196\n",
      "Epoch 713/1500, Loss: 20435.3706\n",
      "Epoch 714/1500, Loss: 20401.6951\n",
      "Epoch 715/1500, Loss: 20368.0945\n",
      "Epoch 716/1500, Loss: 20334.5593\n",
      "Epoch 717/1500, Loss: 20301.0952\n",
      "Epoch 718/1500, Loss: 20267.7012\n",
      "Epoch 719/1500, Loss: 20234.3826\n",
      "Epoch 720/1500, Loss: 20201.1282\n",
      "Epoch 721/1500, Loss: 20167.9492\n",
      "Epoch 722/1500, Loss: 20134.8455\n",
      "Epoch 723/1500, Loss: 20101.8059\n",
      "Epoch 724/1500, Loss: 20068.8374\n",
      "Epoch 725/1500, Loss: 20035.9399\n",
      "Epoch 726/1500, Loss: 20003.1135\n",
      "Epoch 727/1500, Loss: 19970.3572\n",
      "Epoch 728/1500, Loss: 19937.6687\n",
      "Epoch 729/1500, Loss: 19905.0515\n",
      "Epoch 730/1500, Loss: 19872.5007\n",
      "Epoch 731/1500, Loss: 19840.0244\n",
      "Epoch 732/1500, Loss: 19807.6174\n",
      "Epoch 733/1500, Loss: 19775.2810\n",
      "Epoch 734/1500, Loss: 19743.0122\n",
      "Epoch 735/1500, Loss: 19710.8115\n",
      "Epoch 736/1500, Loss: 19678.6797\n",
      "Epoch 737/1500, Loss: 19646.6191\n",
      "Epoch 738/1500, Loss: 19614.6333\n",
      "Epoch 739/1500, Loss: 19582.7070\n",
      "Epoch 740/1500, Loss: 19550.8523\n",
      "Epoch 741/1500, Loss: 19519.0669\n",
      "Epoch 742/1500, Loss: 19487.3469\n",
      "Epoch 743/1500, Loss: 19455.7000\n",
      "Epoch 744/1500, Loss: 19424.1201\n",
      "Epoch 745/1500, Loss: 19392.6104\n",
      "Epoch 746/1500, Loss: 19361.1680\n",
      "Epoch 747/1500, Loss: 19329.7949\n",
      "Epoch 748/1500, Loss: 19298.4885\n",
      "Epoch 749/1500, Loss: 19267.2456\n",
      "Epoch 750/1500, Loss: 19236.0801\n",
      "Epoch 751/1500, Loss: 19204.9736\n",
      "Epoch 752/1500, Loss: 19173.9390\n",
      "Epoch 753/1500, Loss: 19142.9666\n",
      "Epoch 754/1500, Loss: 19112.0691\n",
      "Epoch 755/1500, Loss: 19081.2393\n",
      "Epoch 756/1500, Loss: 19050.4746\n",
      "Epoch 757/1500, Loss: 19019.7756\n",
      "Epoch 758/1500, Loss: 18989.1453\n",
      "Epoch 759/1500, Loss: 18958.5811\n",
      "Epoch 760/1500, Loss: 18928.0833\n",
      "Epoch 761/1500, Loss: 18897.6526\n",
      "Epoch 762/1500, Loss: 18867.2922\n",
      "Epoch 763/1500, Loss: 18836.9951\n",
      "Epoch 764/1500, Loss: 18806.7656\n",
      "Epoch 765/1500, Loss: 18776.6008\n",
      "Epoch 766/1500, Loss: 18746.5037\n",
      "Epoch 767/1500, Loss: 18716.4722\n",
      "Epoch 768/1500, Loss: 18686.5073\n",
      "Epoch 769/1500, Loss: 18656.6118\n",
      "Epoch 770/1500, Loss: 18626.7742\n",
      "Epoch 771/1500, Loss: 18597.0095\n",
      "Epoch 772/1500, Loss: 18567.3105\n",
      "Epoch 773/1500, Loss: 18537.6741\n",
      "Epoch 774/1500, Loss: 18508.1086\n",
      "Epoch 775/1500, Loss: 18478.5991\n",
      "Epoch 776/1500, Loss: 18449.1631\n",
      "Epoch 777/1500, Loss: 18419.7900\n",
      "Epoch 778/1500, Loss: 18390.4841\n",
      "Epoch 779/1500, Loss: 18361.2378\n",
      "Epoch 780/1500, Loss: 18332.0613\n",
      "Epoch 781/1500, Loss: 18302.9460\n",
      "Epoch 782/1500, Loss: 18273.8997\n",
      "Epoch 783/1500, Loss: 18244.9199\n",
      "Epoch 784/1500, Loss: 18216.0012\n",
      "Epoch 785/1500, Loss: 18187.1453\n",
      "Epoch 786/1500, Loss: 18158.3550\n",
      "Epoch 787/1500, Loss: 18129.6343\n",
      "Epoch 788/1500, Loss: 18100.9727\n",
      "Epoch 789/1500, Loss: 18072.3784\n",
      "Epoch 790/1500, Loss: 18043.8445\n",
      "Epoch 791/1500, Loss: 18015.3743\n",
      "Epoch 792/1500, Loss: 17986.9734\n",
      "Epoch 793/1500, Loss: 17958.6338\n",
      "Epoch 794/1500, Loss: 17930.3560\n",
      "Epoch 795/1500, Loss: 17902.1389\n",
      "Epoch 796/1500, Loss: 17873.9927\n",
      "Epoch 797/1500, Loss: 17845.9097\n",
      "Epoch 798/1500, Loss: 17817.8870\n",
      "Epoch 799/1500, Loss: 17789.9238\n",
      "Epoch 800/1500, Loss: 17762.0298\n",
      "Epoch 801/1500, Loss: 17734.1968\n",
      "Epoch 802/1500, Loss: 17706.4287\n",
      "Epoch 803/1500, Loss: 17678.7178\n",
      "Epoch 804/1500, Loss: 17651.0713\n",
      "Epoch 805/1500, Loss: 17623.4949\n",
      "Epoch 806/1500, Loss: 17595.9722\n",
      "Epoch 807/1500, Loss: 17568.5171\n",
      "Epoch 808/1500, Loss: 17541.1260\n",
      "Epoch 809/1500, Loss: 17513.7954\n",
      "Epoch 810/1500, Loss: 17486.5233\n",
      "Epoch 811/1500, Loss: 17459.3138\n",
      "Epoch 812/1500, Loss: 17432.1748\n",
      "Epoch 813/1500, Loss: 17405.0853\n",
      "Epoch 814/1500, Loss: 17378.0680\n",
      "Epoch 815/1500, Loss: 17351.1034\n",
      "Epoch 816/1500, Loss: 17324.2106\n",
      "Epoch 817/1500, Loss: 17297.3743\n",
      "Epoch 818/1500, Loss: 17270.6002\n",
      "Epoch 819/1500, Loss: 17243.8854\n",
      "Epoch 820/1500, Loss: 17217.2335\n",
      "Epoch 821/1500, Loss: 17190.6447\n",
      "Epoch 822/1500, Loss: 17164.1130\n",
      "Epoch 823/1500, Loss: 17137.6479\n",
      "Epoch 824/1500, Loss: 17111.2391\n",
      "Epoch 825/1500, Loss: 17084.8925\n",
      "Epoch 826/1500, Loss: 17058.6051\n",
      "Epoch 827/1500, Loss: 17032.3838\n",
      "Epoch 828/1500, Loss: 17006.2180\n",
      "Epoch 829/1500, Loss: 16980.1141\n",
      "Epoch 830/1500, Loss: 16954.0677\n",
      "Epoch 831/1500, Loss: 16928.0851\n",
      "Epoch 832/1500, Loss: 16902.1630\n",
      "Epoch 833/1500, Loss: 16876.3011\n",
      "Epoch 834/1500, Loss: 16850.4978\n",
      "Epoch 835/1500, Loss: 16824.7562\n",
      "Epoch 836/1500, Loss: 16799.0707\n",
      "Epoch 837/1500, Loss: 16773.4485\n",
      "Epoch 838/1500, Loss: 16747.8844\n",
      "Epoch 839/1500, Loss: 16722.3817\n",
      "Epoch 840/1500, Loss: 16696.9362\n",
      "Epoch 841/1500, Loss: 16671.5510\n",
      "Epoch 842/1500, Loss: 16646.2291\n",
      "Epoch 843/1500, Loss: 16620.9619\n",
      "Epoch 844/1500, Loss: 16595.7538\n",
      "Epoch 845/1500, Loss: 16570.6064\n",
      "Epoch 846/1500, Loss: 16545.5184\n",
      "Epoch 847/1500, Loss: 16520.4851\n",
      "Epoch 848/1500, Loss: 16495.5140\n",
      "Epoch 849/1500, Loss: 16470.6019\n",
      "Epoch 850/1500, Loss: 16445.7485\n",
      "Epoch 851/1500, Loss: 16420.9508\n",
      "Epoch 852/1500, Loss: 16396.2129\n",
      "Epoch 853/1500, Loss: 16371.5358\n",
      "Epoch 854/1500, Loss: 16346.9139\n",
      "Epoch 855/1500, Loss: 16322.3517\n",
      "Epoch 856/1500, Loss: 16297.8461\n",
      "Epoch 857/1500, Loss: 16273.4015\n",
      "Epoch 858/1500, Loss: 16249.0138\n",
      "Epoch 859/1500, Loss: 16224.6818\n",
      "Epoch 860/1500, Loss: 16200.4088\n",
      "Epoch 861/1500, Loss: 16176.1938\n",
      "Epoch 862/1500, Loss: 16152.0370\n",
      "Epoch 863/1500, Loss: 16127.9359\n",
      "Epoch 864/1500, Loss: 16103.8925\n",
      "Epoch 865/1500, Loss: 16079.9050\n",
      "Epoch 866/1500, Loss: 16055.9774\n",
      "Epoch 867/1500, Loss: 16032.1050\n",
      "Epoch 868/1500, Loss: 16008.2911\n",
      "Epoch 869/1500, Loss: 15984.5332\n",
      "Epoch 870/1500, Loss: 15960.8296\n",
      "Epoch 871/1500, Loss: 15937.1888\n",
      "Epoch 872/1500, Loss: 15913.6012\n",
      "Epoch 873/1500, Loss: 15890.0701\n",
      "Epoch 874/1500, Loss: 15866.5948\n",
      "Epoch 875/1500, Loss: 15843.1776\n",
      "Epoch 876/1500, Loss: 15819.8156\n",
      "Epoch 877/1500, Loss: 15796.5093\n",
      "Epoch 878/1500, Loss: 15773.2610\n",
      "Epoch 879/1500, Loss: 15750.0651\n",
      "Epoch 880/1500, Loss: 15726.9264\n",
      "Epoch 881/1500, Loss: 15703.8457\n",
      "Epoch 882/1500, Loss: 15680.8224\n",
      "Epoch 883/1500, Loss: 15657.8503\n",
      "Epoch 884/1500, Loss: 15634.9332\n",
      "Epoch 885/1500, Loss: 15612.0729\n",
      "Epoch 886/1500, Loss: 15589.2686\n",
      "Epoch 887/1500, Loss: 15566.5211\n",
      "Epoch 888/1500, Loss: 15543.8263\n",
      "Epoch 889/1500, Loss: 15521.1913\n",
      "Epoch 890/1500, Loss: 15498.6053\n",
      "Epoch 891/1500, Loss: 15476.0786\n",
      "Epoch 892/1500, Loss: 15453.6036\n",
      "Epoch 893/1500, Loss: 15431.1859\n",
      "Epoch 894/1500, Loss: 15408.8236\n",
      "Epoch 895/1500, Loss: 15386.5137\n",
      "Epoch 896/1500, Loss: 15364.2596\n",
      "Epoch 897/1500, Loss: 15342.0551\n",
      "Epoch 898/1500, Loss: 15319.9071\n",
      "Epoch 899/1500, Loss: 15297.8169\n",
      "Epoch 900/1500, Loss: 15275.7804\n",
      "Epoch 901/1500, Loss: 15253.7942\n",
      "Epoch 902/1500, Loss: 15231.8677\n",
      "Epoch 903/1500, Loss: 15209.9917\n",
      "Epoch 904/1500, Loss: 15188.1691\n",
      "Epoch 905/1500, Loss: 15166.3989\n",
      "Epoch 906/1500, Loss: 15144.6847\n",
      "Epoch 907/1500, Loss: 15123.0217\n",
      "Epoch 908/1500, Loss: 15101.4199\n",
      "Epoch 909/1500, Loss: 15079.8613\n",
      "Epoch 910/1500, Loss: 15058.3621\n",
      "Epoch 911/1500, Loss: 15036.9138\n",
      "Epoch 912/1500, Loss: 15015.5193\n",
      "Epoch 913/1500, Loss: 14994.1780\n",
      "Epoch 914/1500, Loss: 14972.8873\n",
      "Epoch 915/1500, Loss: 14951.6533\n",
      "Epoch 916/1500, Loss: 14930.4698\n",
      "Epoch 917/1500, Loss: 14909.3411\n",
      "Epoch 918/1500, Loss: 14888.2616\n",
      "Epoch 919/1500, Loss: 14867.2345\n",
      "Epoch 920/1500, Loss: 14846.2628\n",
      "Epoch 921/1500, Loss: 14825.3428\n",
      "Epoch 922/1500, Loss: 14804.4745\n",
      "Epoch 923/1500, Loss: 14783.6586\n",
      "Epoch 924/1500, Loss: 14762.8956\n",
      "Epoch 925/1500, Loss: 14742.1814\n",
      "Epoch 926/1500, Loss: 14721.5189\n",
      "Epoch 927/1500, Loss: 14700.9115\n",
      "Epoch 928/1500, Loss: 14680.3568\n",
      "Epoch 929/1500, Loss: 14659.8507\n",
      "Epoch 930/1500, Loss: 14639.3961\n",
      "Epoch 931/1500, Loss: 14618.9956\n",
      "Epoch 932/1500, Loss: 14598.6478\n",
      "Epoch 933/1500, Loss: 14578.3468\n",
      "Epoch 934/1500, Loss: 14558.0980\n",
      "Epoch 935/1500, Loss: 14537.9000\n",
      "Epoch 936/1500, Loss: 14517.7559\n",
      "Epoch 937/1500, Loss: 14497.6626\n",
      "Epoch 938/1500, Loss: 14477.6188\n",
      "Epoch 939/1500, Loss: 14457.6222\n",
      "Epoch 940/1500, Loss: 14437.6779\n",
      "Epoch 941/1500, Loss: 14417.7885\n",
      "Epoch 942/1500, Loss: 14397.9495\n",
      "Epoch 943/1500, Loss: 14378.1587\n",
      "Epoch 944/1500, Loss: 14358.4161\n",
      "Epoch 945/1500, Loss: 14338.7297\n",
      "Epoch 946/1500, Loss: 14319.0856\n",
      "Epoch 947/1500, Loss: 14299.4958\n",
      "Epoch 948/1500, Loss: 14279.9553\n",
      "Epoch 949/1500, Loss: 14260.4663\n",
      "Epoch 950/1500, Loss: 14241.0261\n",
      "Epoch 951/1500, Loss: 14221.6366\n",
      "Epoch 952/1500, Loss: 14202.2944\n",
      "Epoch 953/1500, Loss: 14183.0046\n",
      "Epoch 954/1500, Loss: 14163.7612\n",
      "Epoch 955/1500, Loss: 14144.5719\n",
      "Epoch 956/1500, Loss: 14125.4264\n",
      "Epoch 957/1500, Loss: 14106.3352\n",
      "Epoch 958/1500, Loss: 14087.2885\n",
      "Epoch 959/1500, Loss: 14068.2943\n",
      "Epoch 960/1500, Loss: 14049.3469\n",
      "Epoch 961/1500, Loss: 14030.4489\n",
      "Epoch 962/1500, Loss: 14011.6018\n",
      "Epoch 963/1500, Loss: 13992.8037\n",
      "Epoch 964/1500, Loss: 13974.0514\n",
      "Epoch 965/1500, Loss: 13955.3496\n",
      "Epoch 966/1500, Loss: 13936.6957\n",
      "Epoch 967/1500, Loss: 13918.0912\n",
      "Epoch 968/1500, Loss: 13899.5317\n",
      "Epoch 969/1500, Loss: 13881.0248\n",
      "Epoch 970/1500, Loss: 13862.5612\n",
      "Epoch 971/1500, Loss: 13844.1519\n",
      "Epoch 972/1500, Loss: 13825.7838\n",
      "Epoch 973/1500, Loss: 13807.4691\n",
      "Epoch 974/1500, Loss: 13789.2001\n",
      "Epoch 975/1500, Loss: 13770.9797\n",
      "Epoch 976/1500, Loss: 13752.8044\n",
      "Epoch 977/1500, Loss: 13734.6799\n",
      "Epoch 978/1500, Loss: 13716.6007\n",
      "Epoch 979/1500, Loss: 13698.5677\n",
      "Epoch 980/1500, Loss: 13680.5880\n",
      "Epoch 981/1500, Loss: 13662.6509\n",
      "Epoch 982/1500, Loss: 13644.7601\n",
      "Epoch 983/1500, Loss: 13626.9186\n",
      "Epoch 984/1500, Loss: 13609.1244\n",
      "Epoch 985/1500, Loss: 13591.3776\n",
      "Epoch 986/1500, Loss: 13573.6743\n",
      "Epoch 987/1500, Loss: 13556.0198\n",
      "Epoch 988/1500, Loss: 13538.4133\n",
      "Epoch 989/1500, Loss: 13520.8511\n",
      "Epoch 990/1500, Loss: 13503.3363\n",
      "Epoch 991/1500, Loss: 13485.8677\n",
      "Epoch 992/1500, Loss: 13468.4472\n",
      "Epoch 993/1500, Loss: 13451.0709\n",
      "Epoch 994/1500, Loss: 13433.7422\n",
      "Epoch 995/1500, Loss: 13416.4598\n",
      "Epoch 996/1500, Loss: 13399.2214\n",
      "Epoch 997/1500, Loss: 13382.0289\n",
      "Epoch 998/1500, Loss: 13364.8848\n",
      "Epoch 999/1500, Loss: 13347.7847\n",
      "Epoch 1000/1500, Loss: 13330.7296\n",
      "Epoch 1001/1500, Loss: 13313.7198\n",
      "Epoch 1002/1500, Loss: 13296.7577\n",
      "Epoch 1003/1500, Loss: 13279.8409\n",
      "Epoch 1004/1500, Loss: 13262.9703\n",
      "Epoch 1005/1500, Loss: 13246.1405\n",
      "Epoch 1006/1500, Loss: 13229.3607\n",
      "Epoch 1007/1500, Loss: 13212.6225\n",
      "Epoch 1008/1500, Loss: 13195.9315\n",
      "Epoch 1009/1500, Loss: 13179.2842\n",
      "Epoch 1010/1500, Loss: 13162.6826\n",
      "Epoch 1011/1500, Loss: 13146.1284\n",
      "Epoch 1012/1500, Loss: 13129.6132\n",
      "Epoch 1013/1500, Loss: 13113.1467\n",
      "Epoch 1014/1500, Loss: 13096.7235\n",
      "Epoch 1015/1500, Loss: 13080.3461\n",
      "Epoch 1016/1500, Loss: 13064.0129\n",
      "Epoch 1017/1500, Loss: 13047.7226\n",
      "Epoch 1018/1500, Loss: 13031.4747\n",
      "Epoch 1019/1500, Loss: 13015.2759\n",
      "Epoch 1020/1500, Loss: 12999.1163\n",
      "Epoch 1021/1500, Loss: 12983.0023\n",
      "Epoch 1022/1500, Loss: 12966.9341\n",
      "Epoch 1023/1500, Loss: 12950.9083\n",
      "Epoch 1024/1500, Loss: 12934.9275\n",
      "Epoch 1025/1500, Loss: 12918.9871\n",
      "Epoch 1026/1500, Loss: 12903.0911\n",
      "Epoch 1027/1500, Loss: 12887.2413\n",
      "Epoch 1028/1500, Loss: 12871.4351\n",
      "Epoch 1029/1500, Loss: 12855.6678\n",
      "Epoch 1030/1500, Loss: 12839.9465\n",
      "Epoch 1031/1500, Loss: 12824.2729\n",
      "Epoch 1032/1500, Loss: 12808.6359\n",
      "Epoch 1033/1500, Loss: 12793.0433\n",
      "Epoch 1034/1500, Loss: 12777.4949\n",
      "Epoch 1035/1500, Loss: 12761.9889\n",
      "Epoch 1036/1500, Loss: 12746.5262\n",
      "Epoch 1037/1500, Loss: 12731.1026\n",
      "Epoch 1038/1500, Loss: 12715.7247\n",
      "Epoch 1039/1500, Loss: 12700.3916\n",
      "Epoch 1040/1500, Loss: 12685.0977\n",
      "Epoch 1041/1500, Loss: 12669.8460\n",
      "Epoch 1042/1500, Loss: 12654.6386\n",
      "Epoch 1043/1500, Loss: 12639.4718\n",
      "Epoch 1044/1500, Loss: 12624.3486\n",
      "Epoch 1045/1500, Loss: 12609.2650\n",
      "Epoch 1046/1500, Loss: 12594.2283\n",
      "Epoch 1047/1500, Loss: 12579.2282\n",
      "Epoch 1048/1500, Loss: 12564.2744\n",
      "Epoch 1049/1500, Loss: 12549.3611\n",
      "Epoch 1050/1500, Loss: 12534.4862\n",
      "Epoch 1051/1500, Loss: 12519.6574\n",
      "Epoch 1052/1500, Loss: 12504.8658\n",
      "Epoch 1053/1500, Loss: 12490.1196\n",
      "Epoch 1054/1500, Loss: 12475.4108\n",
      "Epoch 1055/1500, Loss: 12460.7470\n",
      "Epoch 1056/1500, Loss: 12446.1224\n",
      "Epoch 1057/1500, Loss: 12431.5369\n",
      "Epoch 1058/1500, Loss: 12416.9965\n",
      "Epoch 1059/1500, Loss: 12402.4920\n",
      "Epoch 1060/1500, Loss: 12388.0334\n",
      "Epoch 1061/1500, Loss: 12373.6140\n",
      "Epoch 1062/1500, Loss: 12359.2336\n",
      "Epoch 1063/1500, Loss: 12344.8973\n",
      "Epoch 1064/1500, Loss: 12330.6018\n",
      "Epoch 1065/1500, Loss: 12316.3425\n",
      "Epoch 1066/1500, Loss: 12302.1247\n",
      "Epoch 1067/1500, Loss: 12287.9496\n",
      "Epoch 1068/1500, Loss: 12273.8129\n",
      "Epoch 1069/1500, Loss: 12259.7167\n",
      "Epoch 1070/1500, Loss: 12245.6612\n",
      "Epoch 1071/1500, Loss: 12231.6447\n",
      "Epoch 1072/1500, Loss: 12217.6711\n",
      "Epoch 1073/1500, Loss: 12203.7350\n",
      "Epoch 1074/1500, Loss: 12189.8370\n",
      "Epoch 1075/1500, Loss: 12175.9814\n",
      "Epoch 1076/1500, Loss: 12162.1641\n",
      "Epoch 1077/1500, Loss: 12148.3900\n",
      "Epoch 1078/1500, Loss: 12134.6487\n",
      "Epoch 1079/1500, Loss: 12120.9512\n",
      "Epoch 1080/1500, Loss: 12107.2928\n",
      "Epoch 1081/1500, Loss: 12093.6727\n",
      "Epoch 1082/1500, Loss: 12080.0911\n",
      "Epoch 1083/1500, Loss: 12066.5497\n",
      "Epoch 1084/1500, Loss: 12053.0482\n",
      "Epoch 1085/1500, Loss: 12039.5850\n",
      "Epoch 1086/1500, Loss: 12026.1586\n",
      "Epoch 1087/1500, Loss: 12012.7720\n",
      "Epoch 1088/1500, Loss: 11999.4243\n",
      "Epoch 1089/1500, Loss: 11986.1179\n",
      "Epoch 1090/1500, Loss: 11972.8477\n",
      "Epoch 1091/1500, Loss: 11959.6135\n",
      "Epoch 1092/1500, Loss: 11946.4244\n",
      "Epoch 1093/1500, Loss: 11933.2692\n",
      "Epoch 1094/1500, Loss: 11920.1500\n",
      "Epoch 1095/1500, Loss: 11907.0712\n",
      "Epoch 1096/1500, Loss: 11894.0315\n",
      "Epoch 1097/1500, Loss: 11881.0301\n",
      "Epoch 1098/1500, Loss: 11868.0666\n",
      "Epoch 1099/1500, Loss: 11855.1393\n",
      "Epoch 1100/1500, Loss: 11842.2504\n",
      "Epoch 1101/1500, Loss: 11829.3992\n",
      "Epoch 1102/1500, Loss: 11816.5868\n",
      "Epoch 1103/1500, Loss: 11803.8096\n",
      "Epoch 1104/1500, Loss: 11791.0714\n",
      "Epoch 1105/1500, Loss: 11778.3707\n",
      "Epoch 1106/1500, Loss: 11765.7074\n",
      "Epoch 1107/1500, Loss: 11753.0814\n",
      "Epoch 1108/1500, Loss: 11740.4937\n",
      "Epoch 1109/1500, Loss: 11727.9393\n",
      "Epoch 1110/1500, Loss: 11715.4276\n",
      "Epoch 1111/1500, Loss: 11702.9484\n",
      "Epoch 1112/1500, Loss: 11690.5081\n",
      "Epoch 1113/1500, Loss: 11678.1031\n",
      "Epoch 1114/1500, Loss: 11665.7376\n",
      "Epoch 1115/1500, Loss: 11653.4050\n",
      "Epoch 1116/1500, Loss: 11641.1116\n",
      "Epoch 1117/1500, Loss: 11628.8541\n",
      "Epoch 1118/1500, Loss: 11616.6321\n",
      "Epoch 1119/1500, Loss: 11604.4470\n",
      "Epoch 1120/1500, Loss: 11592.2997\n",
      "Epoch 1121/1500, Loss: 11580.1858\n",
      "Epoch 1122/1500, Loss: 11568.1102\n",
      "Epoch 1123/1500, Loss: 11556.0707\n",
      "Epoch 1124/1500, Loss: 11544.0656\n",
      "Epoch 1125/1500, Loss: 11532.0973\n",
      "Epoch 1126/1500, Loss: 11520.1646\n",
      "Epoch 1127/1500, Loss: 11508.2688\n",
      "Epoch 1128/1500, Loss: 11496.4089\n",
      "Epoch 1129/1500, Loss: 11484.5856\n",
      "Epoch 1130/1500, Loss: 11472.7930\n",
      "Epoch 1131/1500, Loss: 11461.0396\n",
      "Epoch 1132/1500, Loss: 11449.3214\n",
      "Epoch 1133/1500, Loss: 11437.6376\n",
      "Epoch 1134/1500, Loss: 11425.9880\n",
      "Epoch 1135/1500, Loss: 11414.3752\n",
      "Epoch 1136/1500, Loss: 11402.7966\n",
      "Epoch 1137/1500, Loss: 11391.2548\n",
      "Epoch 1138/1500, Loss: 11379.7484\n",
      "Epoch 1139/1500, Loss: 11368.2750\n",
      "Epoch 1140/1500, Loss: 11356.8359\n",
      "Epoch 1141/1500, Loss: 11345.4322\n",
      "Epoch 1142/1500, Loss: 11334.0628\n",
      "Epoch 1143/1500, Loss: 11322.7297\n",
      "Epoch 1144/1500, Loss: 11311.4294\n",
      "Epoch 1145/1500, Loss: 11300.1638\n",
      "Epoch 1146/1500, Loss: 11288.9338\n",
      "Epoch 1147/1500, Loss: 11277.7350\n",
      "Epoch 1148/1500, Loss: 11266.5741\n",
      "Epoch 1149/1500, Loss: 11255.4456\n",
      "Epoch 1150/1500, Loss: 11244.3537\n",
      "Epoch 1151/1500, Loss: 11233.2897\n",
      "Epoch 1152/1500, Loss: 11222.2676\n",
      "Epoch 1153/1500, Loss: 11211.2724\n",
      "Epoch 1154/1500, Loss: 11200.3130\n",
      "Epoch 1155/1500, Loss: 11189.3898\n",
      "Epoch 1156/1500, Loss: 11178.4995\n",
      "Epoch 1157/1500, Loss: 11167.6406\n",
      "Epoch 1158/1500, Loss: 11156.8179\n",
      "Epoch 1159/1500, Loss: 11146.0260\n",
      "Epoch 1160/1500, Loss: 11135.2706\n",
      "Epoch 1161/1500, Loss: 11124.5459\n",
      "Epoch 1162/1500, Loss: 11113.8518\n",
      "Epoch 1163/1500, Loss: 11103.1979\n",
      "Epoch 1164/1500, Loss: 11092.5719\n",
      "Epoch 1165/1500, Loss: 11081.9792\n",
      "Epoch 1166/1500, Loss: 11071.4215\n",
      "Epoch 1167/1500, Loss: 11060.8955\n",
      "Epoch 1168/1500, Loss: 11050.4035\n",
      "Epoch 1169/1500, Loss: 11039.9432\n",
      "Epoch 1170/1500, Loss: 11029.5160\n",
      "Epoch 1171/1500, Loss: 11019.1197\n",
      "Epoch 1172/1500, Loss: 11008.7548\n",
      "Epoch 1173/1500, Loss: 10998.4261\n",
      "Epoch 1174/1500, Loss: 10988.1283\n",
      "Epoch 1175/1500, Loss: 10977.8629\n",
      "Epoch 1176/1500, Loss: 10967.6303\n",
      "Epoch 1177/1500, Loss: 10957.4296\n",
      "Epoch 1178/1500, Loss: 10947.2602\n",
      "Epoch 1179/1500, Loss: 10937.1217\n",
      "Epoch 1180/1500, Loss: 10927.0187\n",
      "Epoch 1181/1500, Loss: 10916.9450\n",
      "Epoch 1182/1500, Loss: 10906.9028\n",
      "Epoch 1183/1500, Loss: 10896.8936\n",
      "Epoch 1184/1500, Loss: 10886.9153\n",
      "Epoch 1185/1500, Loss: 10876.9686\n",
      "Epoch 1186/1500, Loss: 10867.0542\n",
      "Epoch 1187/1500, Loss: 10857.1707\n",
      "Epoch 1188/1500, Loss: 10847.3191\n",
      "Epoch 1189/1500, Loss: 10837.4970\n",
      "Epoch 1190/1500, Loss: 10827.7082\n",
      "Epoch 1191/1500, Loss: 10817.9491\n",
      "Epoch 1192/1500, Loss: 10808.2195\n",
      "Epoch 1193/1500, Loss: 10798.5271\n",
      "Epoch 1194/1500, Loss: 10788.8593\n",
      "Epoch 1195/1500, Loss: 10779.2251\n",
      "Epoch 1196/1500, Loss: 10769.6218\n",
      "Epoch 1197/1500, Loss: 10760.0483\n",
      "Epoch 1198/1500, Loss: 10750.5056\n",
      "Epoch 1199/1500, Loss: 10740.9934\n",
      "Epoch 1200/1500, Loss: 10731.5153\n",
      "Epoch 1201/1500, Loss: 10722.0646\n",
      "Epoch 1202/1500, Loss: 10712.6438\n",
      "Epoch 1203/1500, Loss: 10703.2534\n",
      "Epoch 1204/1500, Loss: 10693.8948\n",
      "Epoch 1205/1500, Loss: 10684.5654\n",
      "Epoch 1206/1500, Loss: 10675.2639\n",
      "Epoch 1207/1500, Loss: 10665.9966\n",
      "Epoch 1208/1500, Loss: 10656.7570\n",
      "Epoch 1209/1500, Loss: 10647.5474\n",
      "Epoch 1210/1500, Loss: 10638.3677\n",
      "Epoch 1211/1500, Loss: 10629.2178\n",
      "Epoch 1212/1500, Loss: 10620.0977\n",
      "Epoch 1213/1500, Loss: 10611.0066\n",
      "Epoch 1214/1500, Loss: 10601.9469\n",
      "Epoch 1215/1500, Loss: 10592.9171\n",
      "Epoch 1216/1500, Loss: 10583.9149\n",
      "Epoch 1217/1500, Loss: 10574.9394\n",
      "Epoch 1218/1500, Loss: 10565.9976\n",
      "Epoch 1219/1500, Loss: 10557.0833\n",
      "Epoch 1220/1500, Loss: 10548.1986\n",
      "Epoch 1221/1500, Loss: 10539.3423\n",
      "Epoch 1222/1500, Loss: 10530.5164\n",
      "Epoch 1223/1500, Loss: 10521.7164\n",
      "Epoch 1224/1500, Loss: 10512.9484\n",
      "Epoch 1225/1500, Loss: 10504.2079\n",
      "Epoch 1226/1500, Loss: 10495.4986\n",
      "Epoch 1227/1500, Loss: 10486.8159\n",
      "Epoch 1228/1500, Loss: 10478.1602\n",
      "Epoch 1229/1500, Loss: 10469.5359\n",
      "Epoch 1230/1500, Loss: 10460.9387\n",
      "Epoch 1231/1500, Loss: 10452.3709\n",
      "Epoch 1232/1500, Loss: 10443.8330\n",
      "Epoch 1233/1500, Loss: 10435.3186\n",
      "Epoch 1234/1500, Loss: 10426.8349\n",
      "Epoch 1235/1500, Loss: 10418.3805\n",
      "Epoch 1236/1500, Loss: 10409.9531\n",
      "Epoch 1237/1500, Loss: 10401.5525\n",
      "Epoch 1238/1500, Loss: 10393.1811\n",
      "Epoch 1239/1500, Loss: 10384.8395\n",
      "Epoch 1240/1500, Loss: 10376.5228\n",
      "Epoch 1241/1500, Loss: 10368.2341\n",
      "Epoch 1242/1500, Loss: 10359.9741\n",
      "Epoch 1243/1500, Loss: 10351.7408\n",
      "Epoch 1244/1500, Loss: 10343.5365\n",
      "Epoch 1245/1500, Loss: 10335.3588\n",
      "Epoch 1246/1500, Loss: 10327.2099\n",
      "Epoch 1247/1500, Loss: 10319.0874\n",
      "Epoch 1248/1500, Loss: 10310.9908\n",
      "Epoch 1249/1500, Loss: 10302.9239\n",
      "Epoch 1250/1500, Loss: 10294.8833\n",
      "Epoch 1251/1500, Loss: 10286.8684\n",
      "Epoch 1252/1500, Loss: 10278.8819\n",
      "Epoch 1253/1500, Loss: 10270.9231\n",
      "Epoch 1254/1500, Loss: 10262.9905\n",
      "Epoch 1255/1500, Loss: 10255.0835\n",
      "Epoch 1256/1500, Loss: 10247.2029\n",
      "Epoch 1257/1500, Loss: 10239.3506\n",
      "Epoch 1258/1500, Loss: 10231.5236\n",
      "Epoch 1259/1500, Loss: 10223.7259\n",
      "Epoch 1260/1500, Loss: 10215.9525\n",
      "Epoch 1261/1500, Loss: 10208.2072\n",
      "Epoch 1262/1500, Loss: 10200.4863\n",
      "Epoch 1263/1500, Loss: 10192.7924\n",
      "Epoch 1264/1500, Loss: 10185.1256\n",
      "Epoch 1265/1500, Loss: 10177.4821\n",
      "Epoch 1266/1500, Loss: 10169.8667\n",
      "Epoch 1267/1500, Loss: 10162.2792\n",
      "Epoch 1268/1500, Loss: 10154.7148\n",
      "Epoch 1269/1500, Loss: 10147.1764\n",
      "Epoch 1270/1500, Loss: 10139.6688\n",
      "Epoch 1271/1500, Loss: 10132.1821\n",
      "Epoch 1272/1500, Loss: 10124.7215\n",
      "Epoch 1273/1500, Loss: 10117.2876\n",
      "Epoch 1274/1500, Loss: 10109.8787\n",
      "Epoch 1275/1500, Loss: 10102.4955\n",
      "Epoch 1276/1500, Loss: 10095.1380\n",
      "Epoch 1277/1500, Loss: 10087.8073\n",
      "Epoch 1278/1500, Loss: 10080.4994\n",
      "Epoch 1279/1500, Loss: 10073.2200\n",
      "Epoch 1280/1500, Loss: 10065.9613\n",
      "Epoch 1281/1500, Loss: 10058.7313\n",
      "Epoch 1282/1500, Loss: 10051.5268\n",
      "Epoch 1283/1500, Loss: 10044.3449\n",
      "Epoch 1284/1500, Loss: 10037.1886\n",
      "Epoch 1285/1500, Loss: 10030.0597\n",
      "Epoch 1286/1500, Loss: 10022.9513\n",
      "Epoch 1287/1500, Loss: 10015.8703\n",
      "Epoch 1288/1500, Loss: 10008.8147\n",
      "Epoch 1289/1500, Loss: 10001.7834\n",
      "Epoch 1290/1500, Loss: 9994.7746\n",
      "Epoch 1291/1500, Loss: 9987.7921\n",
      "Epoch 1292/1500, Loss: 9980.8339\n",
      "Epoch 1293/1500, Loss: 9973.9018\n",
      "Epoch 1294/1500, Loss: 9966.9940\n",
      "Epoch 1295/1500, Loss: 9960.1054\n",
      "Epoch 1296/1500, Loss: 9953.2441\n",
      "Epoch 1297/1500, Loss: 9946.4078\n",
      "Epoch 1298/1500, Loss: 9939.5966\n",
      "Epoch 1299/1500, Loss: 9932.8076\n",
      "Epoch 1300/1500, Loss: 9926.0425\n",
      "Epoch 1301/1500, Loss: 9919.3015\n",
      "Epoch 1302/1500, Loss: 9912.5854\n",
      "Epoch 1303/1500, Loss: 9905.8904\n",
      "Epoch 1304/1500, Loss: 9899.2222\n",
      "Epoch 1305/1500, Loss: 9892.5780\n",
      "Epoch 1306/1500, Loss: 9885.9556\n",
      "Epoch 1307/1500, Loss: 9879.3571\n",
      "Epoch 1308/1500, Loss: 9872.7824\n",
      "Epoch 1309/1500, Loss: 9866.2295\n",
      "Epoch 1310/1500, Loss: 9859.7023\n",
      "Epoch 1311/1500, Loss: 9853.1969\n",
      "Epoch 1312/1500, Loss: 9846.7162\n",
      "Epoch 1313/1500, Loss: 9840.2571\n",
      "Epoch 1314/1500, Loss: 9833.8227\n",
      "Epoch 1315/1500, Loss: 9827.4099\n",
      "Epoch 1316/1500, Loss: 9821.0197\n",
      "Epoch 1317/1500, Loss: 9814.6550\n",
      "Epoch 1318/1500, Loss: 9808.3098\n",
      "Epoch 1319/1500, Loss: 9801.9902\n",
      "Epoch 1320/1500, Loss: 9795.6940\n",
      "Epoch 1321/1500, Loss: 9789.4173\n",
      "Epoch 1322/1500, Loss: 9783.1669\n",
      "Epoch 1323/1500, Loss: 9776.9370\n",
      "Epoch 1324/1500, Loss: 9770.7274\n",
      "Epoch 1325/1500, Loss: 9764.5441\n",
      "Epoch 1326/1500, Loss: 9758.3812\n",
      "Epoch 1327/1500, Loss: 9752.2434\n",
      "Epoch 1328/1500, Loss: 9746.1240\n",
      "Epoch 1329/1500, Loss: 9740.0287\n",
      "Epoch 1330/1500, Loss: 9733.9547\n",
      "Epoch 1331/1500, Loss: 9727.9038\n",
      "Epoch 1332/1500, Loss: 9721.8741\n",
      "Epoch 1333/1500, Loss: 9715.8674\n",
      "Epoch 1334/1500, Loss: 9709.8818\n",
      "Epoch 1335/1500, Loss: 9703.9193\n",
      "Epoch 1336/1500, Loss: 9697.9768\n",
      "Epoch 1337/1500, Loss: 9692.0563\n",
      "Epoch 1338/1500, Loss: 9686.1587\n",
      "Epoch 1339/1500, Loss: 9680.2841\n",
      "Epoch 1340/1500, Loss: 9674.4274\n",
      "Epoch 1341/1500, Loss: 9668.5945\n",
      "Epoch 1342/1500, Loss: 9662.7815\n",
      "Epoch 1343/1500, Loss: 9656.9914\n",
      "Epoch 1344/1500, Loss: 9651.2220\n",
      "Epoch 1345/1500, Loss: 9645.4754\n",
      "Epoch 1346/1500, Loss: 9639.7495\n",
      "Epoch 1347/1500, Loss: 9634.0424\n",
      "Epoch 1348/1500, Loss: 9628.3580\n",
      "Epoch 1349/1500, Loss: 9622.6951\n",
      "Epoch 1350/1500, Loss: 9617.0530\n",
      "Epoch 1351/1500, Loss: 9611.4315\n",
      "Epoch 1352/1500, Loss: 9605.8316\n",
      "Epoch 1353/1500, Loss: 9600.2502\n",
      "Epoch 1354/1500, Loss: 9594.6923\n",
      "Epoch 1355/1500, Loss: 9589.1538\n",
      "Epoch 1356/1500, Loss: 9583.6369\n",
      "Epoch 1357/1500, Loss: 9578.1415\n",
      "Epoch 1358/1500, Loss: 9572.6655\n",
      "Epoch 1359/1500, Loss: 9567.2079\n",
      "Epoch 1360/1500, Loss: 9561.7726\n",
      "Epoch 1361/1500, Loss: 9556.3585\n",
      "Epoch 1362/1500, Loss: 9550.9630\n",
      "Epoch 1363/1500, Loss: 9545.5887\n",
      "Epoch 1364/1500, Loss: 9540.2345\n",
      "Epoch 1365/1500, Loss: 9534.9018\n",
      "Epoch 1366/1500, Loss: 9529.5870\n",
      "Epoch 1367/1500, Loss: 9524.2936\n",
      "Epoch 1368/1500, Loss: 9519.0192\n",
      "Epoch 1369/1500, Loss: 9513.7652\n",
      "Epoch 1370/1500, Loss: 9508.5321\n",
      "Epoch 1371/1500, Loss: 9503.3180\n",
      "Epoch 1372/1500, Loss: 9498.1222\n",
      "Epoch 1373/1500, Loss: 9492.9472\n",
      "Epoch 1374/1500, Loss: 9487.7923\n",
      "Epoch 1375/1500, Loss: 9482.6554\n",
      "Epoch 1376/1500, Loss: 9477.5413\n",
      "Epoch 1377/1500, Loss: 9472.4423\n",
      "Epoch 1378/1500, Loss: 9467.3661\n",
      "Epoch 1379/1500, Loss: 9462.3087\n",
      "Epoch 1380/1500, Loss: 9457.2714\n",
      "Epoch 1381/1500, Loss: 9452.2516\n",
      "Epoch 1382/1500, Loss: 9447.2508\n",
      "Epoch 1383/1500, Loss: 9442.2717\n",
      "Epoch 1384/1500, Loss: 9437.3093\n",
      "Epoch 1385/1500, Loss: 9432.3657\n",
      "Epoch 1386/1500, Loss: 9427.4418\n",
      "Epoch 1387/1500, Loss: 9422.5365\n",
      "Epoch 1388/1500, Loss: 9417.6508\n",
      "Epoch 1389/1500, Loss: 9412.7829\n",
      "Epoch 1390/1500, Loss: 9407.9345\n",
      "Epoch 1391/1500, Loss: 9403.1064\n",
      "Epoch 1392/1500, Loss: 9398.2942\n",
      "Epoch 1393/1500, Loss: 9393.5034\n",
      "Epoch 1394/1500, Loss: 9388.7281\n",
      "Epoch 1395/1500, Loss: 9383.9723\n",
      "Epoch 1396/1500, Loss: 9379.2360\n",
      "Epoch 1397/1500, Loss: 9374.5170\n",
      "Epoch 1398/1500, Loss: 9369.8164\n",
      "Epoch 1399/1500, Loss: 9365.1360\n",
      "Epoch 1400/1500, Loss: 9360.4721\n",
      "Epoch 1401/1500, Loss: 9355.8247\n",
      "Epoch 1402/1500, Loss: 9351.1983\n",
      "Epoch 1403/1500, Loss: 9346.5894\n",
      "Epoch 1404/1500, Loss: 9341.9966\n",
      "Epoch 1405/1500, Loss: 9337.4250\n",
      "Epoch 1406/1500, Loss: 9332.8697\n",
      "Epoch 1407/1500, Loss: 9328.3304\n",
      "Epoch 1408/1500, Loss: 9323.8116\n",
      "Epoch 1409/1500, Loss: 9319.3086\n",
      "Epoch 1410/1500, Loss: 9314.8248\n",
      "Epoch 1411/1500, Loss: 9310.3562\n",
      "Epoch 1412/1500, Loss: 9305.9093\n",
      "Epoch 1413/1500, Loss: 9301.4786\n",
      "Epoch 1414/1500, Loss: 9297.0631\n",
      "Epoch 1415/1500, Loss: 9292.6683\n",
      "Epoch 1416/1500, Loss: 9288.2867\n",
      "Epoch 1417/1500, Loss: 9283.9259\n",
      "Epoch 1418/1500, Loss: 9279.5800\n",
      "Epoch 1419/1500, Loss: 9275.2540\n",
      "Epoch 1420/1500, Loss: 9270.9438\n",
      "Epoch 1421/1500, Loss: 9266.6514\n",
      "Epoch 1422/1500, Loss: 9262.3751\n",
      "Epoch 1423/1500, Loss: 9258.1134\n",
      "Epoch 1424/1500, Loss: 9253.8734\n",
      "Epoch 1425/1500, Loss: 9249.6472\n",
      "Epoch 1426/1500, Loss: 9245.4388\n",
      "Epoch 1427/1500, Loss: 9241.2480\n",
      "Epoch 1428/1500, Loss: 9237.0738\n",
      "Epoch 1429/1500, Loss: 9232.9153\n",
      "Epoch 1430/1500, Loss: 9228.7755\n",
      "Epoch 1431/1500, Loss: 9224.6505\n",
      "Epoch 1432/1500, Loss: 9220.5409\n",
      "Epoch 1433/1500, Loss: 9216.4507\n",
      "Epoch 1434/1500, Loss: 9212.3763\n",
      "Epoch 1435/1500, Loss: 9208.3171\n",
      "Epoch 1436/1500, Loss: 9204.2735\n",
      "Epoch 1437/1500, Loss: 9200.2496\n",
      "Epoch 1438/1500, Loss: 9196.2379\n",
      "Epoch 1439/1500, Loss: 9192.2455\n",
      "Epoch 1440/1500, Loss: 9188.2679\n",
      "Epoch 1441/1500, Loss: 9184.3075\n",
      "Epoch 1442/1500, Loss: 9180.3612\n",
      "Epoch 1443/1500, Loss: 9176.4336\n",
      "Epoch 1444/1500, Loss: 9172.5191\n",
      "Epoch 1445/1500, Loss: 9168.6201\n",
      "Epoch 1446/1500, Loss: 9164.7382\n",
      "Epoch 1447/1500, Loss: 9160.8745\n",
      "Epoch 1448/1500, Loss: 9157.0261\n",
      "Epoch 1449/1500, Loss: 9153.1909\n",
      "Epoch 1450/1500, Loss: 9149.3718\n",
      "Epoch 1451/1500, Loss: 9145.5700\n",
      "Epoch 1452/1500, Loss: 9141.7831\n",
      "Epoch 1453/1500, Loss: 9138.0099\n",
      "Epoch 1454/1500, Loss: 9134.2542\n",
      "Epoch 1455/1500, Loss: 9130.5135\n",
      "Epoch 1456/1500, Loss: 9126.7891\n",
      "Epoch 1457/1500, Loss: 9123.0787\n",
      "Epoch 1458/1500, Loss: 9119.3822\n",
      "Epoch 1459/1500, Loss: 9115.7039\n",
      "Epoch 1460/1500, Loss: 9112.0395\n",
      "Epoch 1461/1500, Loss: 9108.3909\n",
      "Epoch 1462/1500, Loss: 9104.7562\n",
      "Epoch 1463/1500, Loss: 9101.1375\n",
      "Epoch 1464/1500, Loss: 9097.5327\n",
      "Epoch 1465/1500, Loss: 9093.9447\n",
      "Epoch 1466/1500, Loss: 9090.3715\n",
      "Epoch 1467/1500, Loss: 9086.8111\n",
      "Epoch 1468/1500, Loss: 9083.2659\n",
      "Epoch 1469/1500, Loss: 9079.7347\n",
      "Epoch 1470/1500, Loss: 9076.2217\n",
      "Epoch 1471/1500, Loss: 9072.7203\n",
      "Epoch 1472/1500, Loss: 9069.2345\n",
      "Epoch 1473/1500, Loss: 9065.7639\n",
      "Epoch 1474/1500, Loss: 9062.3066\n",
      "Epoch 1475/1500, Loss: 9058.8650\n",
      "Epoch 1476/1500, Loss: 9055.4360\n",
      "Epoch 1477/1500, Loss: 9052.0236\n",
      "Epoch 1478/1500, Loss: 9048.6237\n",
      "Epoch 1479/1500, Loss: 9045.2385\n",
      "Epoch 1480/1500, Loss: 9041.8677\n",
      "Epoch 1481/1500, Loss: 9038.5125\n",
      "Epoch 1482/1500, Loss: 9035.1707\n",
      "Epoch 1483/1500, Loss: 9031.8419\n",
      "Epoch 1484/1500, Loss: 9028.5288\n",
      "Epoch 1485/1500, Loss: 9025.2276\n",
      "Epoch 1486/1500, Loss: 9021.9416\n",
      "Epoch 1487/1500, Loss: 9018.6689\n",
      "Epoch 1488/1500, Loss: 9015.4110\n",
      "Epoch 1489/1500, Loss: 9012.1672\n",
      "Epoch 1490/1500, Loss: 9008.9358\n",
      "Epoch 1491/1500, Loss: 9005.7193\n",
      "Epoch 1492/1500, Loss: 9002.5157\n",
      "Epoch 1493/1500, Loss: 8999.3242\n",
      "Epoch 1494/1500, Loss: 8996.1491\n",
      "Epoch 1495/1500, Loss: 8992.9879\n",
      "Epoch 1496/1500, Loss: 8989.8392\n",
      "Epoch 1497/1500, Loss: 8986.7036\n",
      "Epoch 1498/1500, Loss: 8983.5820\n",
      "Epoch 1499/1500, Loss: 8980.4728\n",
      "Epoch 1500/1500, Loss: 8977.3785\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 1500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        sequence_batch, timeslot_batch, coordinate_batch, user_id_batch = batch\n",
    "        sequence_batch = sequence_batch.to(device)\n",
    "        timeslot_batch = timeslot_batch.to(device)\n",
    "        coordinate_batch = coordinate_batch.to(device)\n",
    "        user_id_batch = user_id_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        timeslot_pred, coord_pred = model(sequence_batch, user_id_batch)\n",
    "\n",
    "        # Calcular pérdida\n",
    "        loss = loss_function(\n",
    "            timeslot_pred, timeslot_batch, coord_pred, coordinate_batch\n",
    "        )\n",
    "\n",
    "        # Backward y optimización\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Timeslot: 228.72743225097656, True: 241.0\n",
      "Predicted Coordinates: [20.409244537353516, 60.40855026245117], True: [31.0, 71.0]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        sequence_batch, timeslot_batch, coordinate_batch, user_id_batch = batch\n",
    "        sequence_batch = sequence_batch.to(device)\n",
    "        timeslot_batch = timeslot_batch.to(device)\n",
    "        coordinate_batch = coordinate_batch.to(device)\n",
    "        user_id_batch = user_id_batch.to(device)\n",
    "        \n",
    "        timeslot_pred, coord_pred = model(sequence_batch, user_id_batch)\n",
    "        \n",
    "        print(f\"Predicted Timeslot: {timeslot_pred[0].item()}, True: {timeslot_batch[0].item()}\")\n",
    "        print(f\"Predicted Coordinates: {coord_pred[0].tolist()}, True: {coordinate_batch[0].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>d</th>\n",
       "      <th>t</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>t_unificado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid  d   t  x   y  t_unificado\n",
       "0    0  0   0  0  40            0\n",
       "1    0  0   1  1  41            1\n",
       "2    0  1  12  2  42           60\n",
       "3    0  1  13  3  43           61\n",
       "4    0  2  24  4  44          120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Timeslot: 228.72743225097656\n",
      "Predicted Coordinates: (20.409244537353516, 60.40855026245117)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparar una consulta específica\n",
    "recent_data = df[df[\"uid\"] == 0][:5]  # Últimos 5 registros del usuario 0\n",
    "display(recent_data)\n",
    "\n",
    "# Crear la secuencia para la consulta\n",
    "sequence_tensor = create_tensor_from_data(recent_data)  # Reutiliza el código para crear el tensor\n",
    "user_id_tensor = torch.tensor([0], dtype=torch.long).to(device)\n",
    "\n",
    "# Hacer la predicción\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    timeslot_pred, coord_pred = model(sequence_tensor.to(device), user_id_tensor)\n",
    "    \n",
    "    print(f\"Predicted Timeslot: {timeslot_pred.item()}\")\n",
    "    print(f\"Predicted Coordinates: ({coord_pred[0, 0].item()}, {coord_pred[0, 1].item()})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
